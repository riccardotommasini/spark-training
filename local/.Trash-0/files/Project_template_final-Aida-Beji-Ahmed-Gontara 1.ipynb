{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7b829de",
   "metadata": {},
   "source": [
    "## Project Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706eacc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e63d5e80-4214-4bbd-a7e3-d883127bd329;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 422ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e63d5e80-4214-4bbd-a7e3-d883127bd329\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/9ms)\n",
      "23/11/06 20:43:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6604fd",
   "metadata": {},
   "source": [
    "Be sure to start the stream on Kafka!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a8ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType, DateType\n",
    "\n",
    "schema = StructType(\n",
    "      [\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"price\", DoubleType(), False),\n",
    "        StructField(\"timestamp\", TimestampType(), False),\n",
    "      ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f14c0d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_server = \"kafka1:9092\"   \n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "lines = (spark.readStream                        # Get the DataStreamReader\n",
    "  .format(\"kafka\")                                 # Specify the source format as \"kafka\"\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_server) # Configure the Kafka server name and port\n",
    "  .option(\"subscribe\", \"stock\")                       # Subscribe to the \"en\" Kafka topic \n",
    "  .option(\"startingOffsets\", \"earliest\")           # The start point when a query is started\n",
    "  .option(\"maxOffsetsPerTrigger\", 100)             # Rate limit on max offsets per trigger interval\n",
    "  .load()\n",
    "  .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\"))\n",
    "# Load the DataFrame\n",
    ")\n",
    "df = lines.select(\"parsed_value.*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322b439",
   "metadata": {},
   "source": [
    "## The assignment starts here\n",
    "\n",
    "You can create a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a19eb",
   "metadata": {},
   "source": [
    "## Select the N most valuable stocks in a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd5d450",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/06 20:43:33 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-475afcb0-fc6d-4c09-8174-1eea67300a45. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|window                                    |5 most valuable stocks and their prices                                    |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|[2023-11-03 14:55:00, 2023-11-03 14:56:00]|[[ISRG, 192.6465], [BBT, 30.8], [FITB, 16.63], [ZION, 24.57], [HCP, 47.08]]|\n",
      "|[2023-11-03 14:56:00, 2023-11-03 14:57:00]|[[PFE, 27.75], [AKAM, 38.76], [ILMN, 49.74], [FLS, 53.4166], [TGT, 63.21]] |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|window                                    |5 most valuable stocks and their prices                                    |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|[2023-11-03 14:57:00, 2023-11-03 14:58:00]|[[LYB, 63.17], [TGT, 68.83], [AGN, 93.13], [PGR, 25.68], [MSFT, 28.85]]    |\n",
      "|[2023-11-03 14:55:00, 2023-11-03 14:56:00]|[[ISRG, 192.6465], [BBT, 30.8], [FITB, 16.63], [ZION, 24.57], [HCP, 47.08]]|\n",
      "|[2023-11-03 14:56:00, 2023-11-03 14:57:00]|[[PFE, 27.75], [AKAM, 38.76], [ILMN, 49.74], [FLS, 53.4166], [TGT, 63.21]] |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|window                                    |5 most valuable stocks and their prices                                    |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|[2023-11-03 14:58:00, 2023-11-03 14:59:00]|[[FRT, 116.94], [HST, 18.5191], [GWW, 254.0], [IQV, 44.33], [SCG, 53.2375]]|\n",
      "|[2023-11-03 14:57:00, 2023-11-03 14:58:00]|[[LYB, 63.17], [TGT, 68.83], [AGN, 93.13], [PGR, 25.68], [MSFT, 28.85]]    |\n",
      "|[2023-11-03 14:55:00, 2023-11-03 14:56:00]|[[ISRG, 192.6465], [BBT, 30.8], [FITB, 16.63], [ZION, 24.57], [HCP, 47.08]]|\n",
      "|[2023-11-03 14:56:00, 2023-11-03 14:57:00]|[[PFE, 27.75], [AKAM, 38.76], [ILMN, 49.74], [FLS, 53.4166], [TGT, 63.21]] |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|window                                    |5 most valuable stocks and their prices                                    |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|[2023-11-03 14:58:00, 2023-11-03 14:59:00]|[[FRT, 116.94], [HST, 18.5191], [GWW, 254.0], [IQV, 44.33], [SCG, 53.2375]]|\n",
      "|[2023-11-03 14:57:00, 2023-11-03 14:58:00]|[[LYB, 63.17], [TGT, 68.83], [AGN, 93.13], [PGR, 25.68], [MSFT, 28.85]]    |\n",
      "|[2023-11-03 14:55:00, 2023-11-03 14:56:00]|[[ISRG, 192.6465], [BBT, 30.8], [FITB, 16.63], [ZION, 24.57], [HCP, 47.08]]|\n",
      "|[2023-11-03 14:56:00, 2023-11-03 14:57:00]|[[PFE, 27.75], [AKAM, 38.76], [ILMN, 49.74], [FLS, 53.4166], [TGT, 63.21]] |\n",
      "|[2023-11-03 14:59:00, 2023-11-03 15:00:00]|[[FDX, 99.85], [HST, 17.46], [CNP, 23.81], [WMB, 33.66], [AMT, 78.3452]]   |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|window                                    |5 most valuable stocks and their prices                                    |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|[2023-11-03 14:58:00, 2023-11-03 14:59:00]|[[FRT, 116.94], [HST, 18.5191], [GWW, 254.0], [IQV, 44.33], [SCG, 53.2375]]|\n",
      "|[2023-11-03 14:57:00, 2023-11-03 14:58:00]|[[LYB, 63.17], [TGT, 68.83], [AGN, 93.13], [PGR, 25.68], [MSFT, 28.85]]    |\n",
      "|[2023-11-03 14:55:00, 2023-11-03 14:56:00]|[[ISRG, 192.6465], [BBT, 30.8], [FITB, 16.63], [ZION, 24.57], [HCP, 47.08]]|\n",
      "|[2023-11-03 14:56:00, 2023-11-03 14:57:00]|[[PFE, 27.75], [AKAM, 38.76], [ILMN, 49.74], [FLS, 53.4166], [TGT, 63.21]] |\n",
      "|[2023-11-03 14:59:00, 2023-11-03 15:00:00]|[[FDX, 99.85], [HST, 17.46], [CNP, 23.81], [WMB, 33.66], [AMT, 78.3452]]   |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|window                                    |5 most valuable stocks and their prices                                    |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|[2023-11-03 14:58:00, 2023-11-03 14:59:00]|[[FRT, 116.94], [HST, 18.5191], [GWW, 254.0], [IQV, 44.33], [SCG, 53.2375]]|\n",
      "|[2023-11-03 14:57:00, 2023-11-03 14:58:00]|[[LYB, 63.17], [TGT, 68.83], [AGN, 93.13], [PGR, 25.68], [MSFT, 28.85]]    |\n",
      "|[2023-11-03 15:00:00, 2023-11-03 15:01:00]|[[MGM, 16.25], [CMCSA, 22.315], [IRM, 29.08], [ALK, 31.4795], [MPC, 35.05]]|\n",
      "|[2023-11-03 14:55:00, 2023-11-03 14:56:00]|[[ISRG, 192.6465], [BBT, 30.8], [FITB, 16.63], [ZION, 24.57], [HCP, 47.08]]|\n",
      "|[2023-11-03 14:56:00, 2023-11-03 14:57:00]|[[PFE, 27.75], [AKAM, 38.76], [ILMN, 49.74], [FLS, 53.4166], [TGT, 63.21]] |\n",
      "|[2023-11-03 14:59:00, 2023-11-03 15:00:00]|[[FDX, 99.85], [HST, 17.46], [CNP, 23.81], [WMB, 33.66], [AMT, 78.3452]]   |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|window                                    |5 most valuable stocks and their prices                                    |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|[2023-11-03 14:58:00, 2023-11-03 14:59:00]|[[FRT, 116.94], [HST, 18.5191], [GWW, 254.0], [IQV, 44.33], [SCG, 53.2375]]|\n",
      "|[2023-11-03 15:01:00, 2023-11-03 15:02:00]|[[OMC, 62.0], [MKC, 70.49], [SLG, 88.21], [BLL, 22.65], [CMA, 42.11]]      |\n",
      "|[2023-11-03 14:57:00, 2023-11-03 14:58:00]|[[LYB, 63.17], [TGT, 68.83], [AGN, 93.13], [PGR, 25.68], [MSFT, 28.85]]    |\n",
      "|[2023-11-03 15:00:00, 2023-11-03 15:01:00]|[[MGM, 16.25], [CMCSA, 22.315], [IRM, 29.08], [ALK, 31.4795], [MPC, 35.05]]|\n",
      "|[2023-11-03 14:55:00, 2023-11-03 14:56:00]|[[ISRG, 192.6465], [BBT, 30.8], [FITB, 16.63], [ZION, 24.57], [HCP, 47.08]]|\n",
      "|[2023-11-03 14:56:00, 2023-11-03 14:57:00]|[[PFE, 27.75], [AKAM, 38.76], [ILMN, 49.74], [FLS, 53.4166], [TGT, 63.21]] |\n",
      "|[2023-11-03 14:59:00, 2023-11-03 15:00:00]|[[FDX, 99.85], [HST, 17.46], [CNP, 23.81], [WMB, 33.66], [AMT, 78.3452]]   |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|window                                    |5 most valuable stocks and their prices                                    |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|[2023-11-03 15:01:00, 2023-11-03 15:02:00]|[[OMC, 62.0], [MKC, 70.49], [SLG, 88.21], [BLL, 22.65], [CMA, 42.11]]      |\n",
      "|[2023-11-03 14:58:00, 2023-11-03 14:59:00]|[[FRT, 116.94], [HST, 18.5191], [GWW, 254.0], [IQV, 44.33], [SCG, 53.2375]]|\n",
      "|[2023-11-03 15:02:00, 2023-11-03 15:03:00]|[[MAA, 63.25], [AES, 13.41], [HOLX, 21.25], [MCO, 71.28], [FFIV, 89.96]]   |\n",
      "|[2023-11-03 14:57:00, 2023-11-03 14:58:00]|[[LYB, 63.17], [TGT, 68.83], [AGN, 93.13], [PGR, 25.68], [MSFT, 28.85]]    |\n",
      "|[2023-11-03 15:00:00, 2023-11-03 15:01:00]|[[MGM, 16.25], [CMCSA, 22.315], [IRM, 29.08], [ALK, 31.4795], [MPC, 35.05]]|\n",
      "|[2023-11-03 14:55:00, 2023-11-03 14:56:00]|[[ISRG, 192.6465], [BBT, 30.8], [FITB, 16.63], [ZION, 24.57], [HCP, 47.08]]|\n",
      "|[2023-11-03 14:56:00, 2023-11-03 14:57:00]|[[PFE, 27.75], [AKAM, 38.76], [ILMN, 49.74], [FLS, 53.4166], [TGT, 63.21]] |\n",
      "|[2023-11-03 14:59:00, 2023-11-03 15:00:00]|[[FDX, 99.85], [HST, 17.46], [CNP, 23.81], [WMB, 33.66], [AMT, 78.3452]]   |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|window                                    |5 most valuable stocks and their prices                                    |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|[2023-11-03 15:01:00, 2023-11-03 15:02:00]|[[OMC, 62.0], [MKC, 70.49], [SLG, 88.21], [BLL, 22.65], [CMA, 42.11]]      |\n",
      "|[2023-11-03 14:58:00, 2023-11-03 14:59:00]|[[FRT, 116.94], [HST, 18.5191], [GWW, 254.0], [IQV, 44.33], [SCG, 53.2375]]|\n",
      "|[2023-11-03 15:02:00, 2023-11-03 15:03:00]|[[MAA, 63.25], [AES, 13.41], [HOLX, 21.25], [MCO, 71.28], [FFIV, 89.96]]   |\n",
      "|[2023-11-03 14:57:00, 2023-11-03 14:58:00]|[[LYB, 63.17], [TGT, 68.83], [AGN, 93.13], [PGR, 25.68], [MSFT, 28.85]]    |\n",
      "|[2023-11-03 15:00:00, 2023-11-03 15:01:00]|[[MGM, 16.25], [CMCSA, 22.315], [IRM, 29.08], [ALK, 31.4795], [MPC, 35.05]]|\n",
      "|[2023-11-03 14:55:00, 2023-11-03 14:56:00]|[[ISRG, 192.6465], [BBT, 30.8], [FITB, 16.63], [ZION, 24.57], [HCP, 47.08]]|\n",
      "|[2023-11-03 15:03:00, 2023-11-03 15:04:00]|[[GILD, 71.25], [HD, 76.8], [GPC, 80.61], [HON, 86.69], [AAP, 100.28]]     |\n",
      "|[2023-11-03 14:56:00, 2023-11-03 14:57:00]|[[PFE, 27.75], [AKAM, 38.76], [ILMN, 49.74], [FLS, 53.4166], [TGT, 63.21]] |\n",
      "|[2023-11-03 14:59:00, 2023-11-03 15:00:00]|[[FDX, 99.85], [HST, 17.46], [CNP, 23.81], [WMB, 33.66], [AMT, 78.3452]]   |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|window                                    |5 most valuable stocks and their prices                                    |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|[2023-11-03 15:01:00, 2023-11-03 15:02:00]|[[OMC, 62.0], [MKC, 70.49], [SLG, 88.21], [BLL, 22.65], [CMA, 42.11]]      |\n",
      "|[2023-11-03 14:58:00, 2023-11-03 14:59:00]|[[FRT, 116.94], [HST, 18.5191], [GWW, 254.0], [IQV, 44.33], [SCG, 53.2375]]|\n",
      "|[2023-11-03 15:02:00, 2023-11-03 15:03:00]|[[MAA, 63.25], [AES, 13.41], [HOLX, 21.25], [MCO, 71.28], [FFIV, 89.96]]   |\n",
      "|[2023-11-03 14:57:00, 2023-11-03 14:58:00]|[[LYB, 63.17], [TGT, 68.83], [AGN, 93.13], [PGR, 25.68], [MSFT, 28.85]]    |\n",
      "|[2023-11-03 15:00:00, 2023-11-03 15:01:00]|[[MGM, 16.25], [CMCSA, 22.315], [IRM, 29.08], [ALK, 31.4795], [MPC, 35.05]]|\n",
      "|[2023-11-03 14:55:00, 2023-11-03 14:56:00]|[[ISRG, 192.6465], [BBT, 30.8], [FITB, 16.63], [ZION, 24.57], [HCP, 47.08]]|\n",
      "|[2023-11-03 15:03:00, 2023-11-03 15:04:00]|[[GILD, 71.25], [HD, 76.8], [GPC, 80.61], [HON, 86.69], [AAP, 100.28]]     |\n",
      "|[2023-11-03 15:04:00, 2023-11-03 15:05:00]|[[JCI, 39.5288], [IR, 57.43], [ESRX, 67.57], [VAR, 76.81], [WDC, 79.8]]    |\n",
      "|[2023-11-03 14:56:00, 2023-11-03 14:57:00]|[[PFE, 27.75], [AKAM, 38.76], [ILMN, 49.74], [FLS, 53.4166], [TGT, 63.21]] |\n",
      "|[2023-11-03 14:59:00, 2023-11-03 15:00:00]|[[FDX, 99.85], [HST, 17.46], [CNP, 23.81], [WMB, 33.66], [AMT, 78.3452]]   |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|window                                    |5 most valuable stocks and their prices                                    |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|[2023-11-03 15:01:00, 2023-11-03 15:02:00]|[[OMC, 62.0], [MKC, 70.49], [SLG, 88.21], [BLL, 22.65], [CMA, 42.11]]      |\n",
      "|[2023-11-03 14:58:00, 2023-11-03 14:59:00]|[[FRT, 116.94], [HST, 18.5191], [GWW, 254.0], [IQV, 44.33], [SCG, 53.2375]]|\n",
      "|[2023-11-03 15:02:00, 2023-11-03 15:03:00]|[[MAA, 63.25], [AES, 13.41], [HOLX, 21.25], [MCO, 71.28], [FFIV, 89.96]]   |\n",
      "|[2023-11-03 14:57:00, 2023-11-03 14:58:00]|[[LYB, 63.17], [TGT, 68.83], [AGN, 93.13], [PGR, 25.68], [MSFT, 28.85]]    |\n",
      "|[2023-11-03 15:00:00, 2023-11-03 15:01:00]|[[MGM, 16.25], [CMCSA, 22.315], [IRM, 29.08], [ALK, 31.4795], [MPC, 35.05]]|\n",
      "|[2023-11-03 14:55:00, 2023-11-03 14:56:00]|[[ISRG, 192.6465], [BBT, 30.8], [FITB, 16.63], [ZION, 24.57], [HCP, 47.08]]|\n",
      "|[2023-11-03 15:03:00, 2023-11-03 15:04:00]|[[GILD, 71.25], [HD, 76.8], [GPC, 80.61], [HON, 86.69], [AAP, 100.28]]     |\n",
      "|[2023-11-03 15:04:00, 2023-11-03 15:05:00]|[[JCI, 39.5288], [IR, 57.43], [ESRX, 67.57], [VAR, 76.81], [WDC, 79.8]]    |\n",
      "|[2023-11-03 14:56:00, 2023-11-03 14:57:00]|[[PFE, 27.75], [AKAM, 38.76], [ILMN, 49.74], [FLS, 53.4166], [TGT, 63.21]] |\n",
      "|[2023-11-03 14:59:00, 2023-11-03 15:00:00]|[[FDX, 99.85], [HST, 17.46], [CNP, 23.81], [WMB, 33.66], [AMT, 78.3452]]   |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|window                                    |5 most valuable stocks and their prices                                    |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "|[2023-11-03 15:01:00, 2023-11-03 15:02:00]|[[OMC, 62.0], [MKC, 70.49], [SLG, 88.21], [BLL, 22.65], [CMA, 42.11]]      |\n",
      "|[2023-11-03 14:58:00, 2023-11-03 14:59:00]|[[FRT, 116.94], [HST, 18.5191], [GWW, 254.0], [IQV, 44.33], [SCG, 53.2375]]|\n",
      "|[2023-11-03 15:02:00, 2023-11-03 15:03:00]|[[MAA, 63.25], [AES, 13.41], [HOLX, 21.25], [MCO, 71.28], [FFIV, 89.96]]   |\n",
      "|[2023-11-03 14:57:00, 2023-11-03 14:58:00]|[[LYB, 63.17], [TGT, 68.83], [AGN, 93.13], [PGR, 25.68], [MSFT, 28.85]]    |\n",
      "|[2023-11-03 15:00:00, 2023-11-03 15:01:00]|[[MGM, 16.25], [CMCSA, 22.315], [IRM, 29.08], [ALK, 31.4795], [MPC, 35.05]]|\n",
      "|[2023-11-03 14:55:00, 2023-11-03 14:56:00]|[[ISRG, 192.6465], [BBT, 30.8], [FITB, 16.63], [ZION, 24.57], [HCP, 47.08]]|\n",
      "|[2023-11-03 15:03:00, 2023-11-03 15:04:00]|[[GILD, 71.25], [HD, 76.8], [GPC, 80.61], [HON, 86.69], [AAP, 100.28]]     |\n",
      "|[2023-11-03 15:04:00, 2023-11-03 15:05:00]|[[JCI, 39.5288], [IR, 57.43], [ESRX, 67.57], [VAR, 76.81], [WDC, 79.8]]    |\n",
      "|[2023-11-03 15:05:00, 2023-11-03 15:06:00]|[[ED, 54.62], [QCOM, 75.86], [SNI, 76.64], [AEE, 36.59], [IDXX, 57.465]]   |\n",
      "|[2023-11-03 14:56:00, 2023-11-03 14:57:00]|[[PFE, 27.75], [AKAM, 38.76], [ILMN, 49.74], [FLS, 53.4166], [TGT, 63.21]] |\n",
      "|[2023-11-03 14:59:00, 2023-11-03 15:00:00]|[[FDX, 99.85], [HST, 17.46], [CNP, 23.81], [WMB, 33.66], [AMT, 78.3452]]   |\n",
      "+------------------------------------------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------------------------------------------------------------------------+\n",
      "|window                                    |5 most valuable stocks and their prices                                     |\n",
      "+------------------------------------------+----------------------------------------------------------------------------+\n",
      "|[2023-11-03 15:01:00, 2023-11-03 15:02:00]|[[OMC, 62.0], [MKC, 70.49], [SLG, 88.21], [BLL, 22.65], [CMA, 42.11]]       |\n",
      "|[2023-11-03 14:58:00, 2023-11-03 14:59:00]|[[FRT, 116.94], [HST, 18.5191], [GWW, 254.0], [IQV, 44.33], [SCG, 53.2375]] |\n",
      "|[2023-11-03 15:02:00, 2023-11-03 15:03:00]|[[MAA, 63.25], [AES, 13.41], [HOLX, 21.25], [MCO, 71.28], [FFIV, 89.96]]    |\n",
      "|[2023-11-03 14:57:00, 2023-11-03 14:58:00]|[[LYB, 63.17], [TGT, 68.83], [AGN, 93.13], [PGR, 25.68], [MSFT, 28.85]]     |\n",
      "|[2023-11-03 15:00:00, 2023-11-03 15:01:00]|[[MGM, 16.25], [CMCSA, 22.315], [IRM, 29.08], [ALK, 31.4795], [MPC, 35.05]] |\n",
      "|[2023-11-03 14:55:00, 2023-11-03 14:56:00]|[[ISRG, 192.6465], [BBT, 30.8], [FITB, 16.63], [ZION, 24.57], [HCP, 47.08]] |\n",
      "|[2023-11-03 15:03:00, 2023-11-03 15:04:00]|[[GILD, 71.25], [HD, 76.8], [GPC, 80.61], [HON, 86.69], [AAP, 100.28]]      |\n",
      "|[2023-11-03 15:04:00, 2023-11-03 15:05:00]|[[JCI, 39.5288], [IR, 57.43], [ESRX, 67.57], [VAR, 76.81], [WDC, 79.8]]     |\n",
      "|[2023-11-03 15:05:00, 2023-11-03 15:06:00]|[[ED, 54.62], [QCOM, 75.86], [SNI, 76.64], [AEE, 36.59], [IDXX, 57.465]]    |\n",
      "|[2023-11-03 14:56:00, 2023-11-03 14:57:00]|[[PFE, 27.75], [AKAM, 38.76], [ILMN, 49.74], [FLS, 53.4166], [TGT, 63.21]]  |\n",
      "|[2023-11-03 15:06:00, 2023-11-03 15:07:00]|[[EQIX, 192.7], [FBHS, 45.08], [GWW, 252.1], [REGN, 343.476], [PAYX, 41.89]]|\n",
      "|[2023-11-03 14:59:00, 2023-11-03 15:00:00]|[[FDX, 99.85], [HST, 17.46], [CNP, 23.81], [WMB, 33.66], [AMT, 78.3452]]    |\n",
      "+------------------------------------------+----------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------------------------------------------------------------------------+\n",
      "|window                                    |5 most valuable stocks and their prices                                     |\n",
      "+------------------------------------------+----------------------------------------------------------------------------+\n",
      "|[2023-11-03 15:01:00, 2023-11-03 15:02:00]|[[OMC, 62.0], [MKC, 70.49], [SLG, 88.21], [BLL, 22.65], [CMA, 42.11]]       |\n",
      "|[2023-11-03 14:58:00, 2023-11-03 14:59:00]|[[FRT, 116.94], [HST, 18.5191], [GWW, 254.0], [IQV, 44.33], [SCG, 53.2375]] |\n",
      "|[2023-11-03 15:07:00, 2023-11-03 15:08:00]|[[TJX, 60.16], [MKC, 71.54], [CME, 74.86], [FLS, 76.58], [EMN, 85.7]]       |\n",
      "|[2023-11-03 15:02:00, 2023-11-03 15:03:00]|[[MAA, 63.25], [AES, 13.41], [HOLX, 21.25], [MCO, 71.28], [FFIV, 89.96]]    |\n",
      "|[2023-11-03 14:57:00, 2023-11-03 14:58:00]|[[LYB, 63.17], [TGT, 68.83], [AGN, 93.13], [PGR, 25.68], [MSFT, 28.85]]     |\n",
      "|[2023-11-03 15:00:00, 2023-11-03 15:01:00]|[[MGM, 16.25], [CMCSA, 22.315], [IRM, 29.08], [ALK, 31.4795], [MPC, 35.05]] |\n",
      "|[2023-11-03 14:55:00, 2023-11-03 14:56:00]|[[ISRG, 192.6465], [BBT, 30.8], [FITB, 16.63], [ZION, 24.57], [HCP, 47.08]] |\n",
      "|[2023-11-03 15:03:00, 2023-11-03 15:04:00]|[[GILD, 71.25], [HD, 76.8], [GPC, 80.61], [HON, 86.69], [AAP, 100.28]]      |\n",
      "|[2023-11-03 15:04:00, 2023-11-03 15:05:00]|[[JCI, 39.5288], [IR, 57.43], [ESRX, 67.57], [VAR, 76.81], [WDC, 79.8]]     |\n",
      "|[2023-11-03 15:05:00, 2023-11-03 15:06:00]|[[ED, 54.62], [QCOM, 75.86], [SNI, 76.64], [AEE, 36.59], [IDXX, 57.465]]    |\n",
      "|[2023-11-03 14:56:00, 2023-11-03 14:57:00]|[[PFE, 27.75], [AKAM, 38.76], [ILMN, 49.74], [FLS, 53.4166], [TGT, 63.21]]  |\n",
      "|[2023-11-03 15:06:00, 2023-11-03 15:07:00]|[[EQIX, 192.7], [FBHS, 45.08], [GWW, 252.1], [REGN, 343.476], [PAYX, 41.89]]|\n",
      "|[2023-11-03 14:59:00, 2023-11-03 15:00:00]|[[FDX, 99.85], [HST, 17.46], [CNP, 23.81], [WMB, 33.66], [AMT, 78.3452]]    |\n",
      "+------------------------------------------+----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, desc, col, collect_list, struct, slice\n",
    "\n",
    "# Define a 1-minute sliding time window and collect prices within each window\n",
    "windowed_df = df.groupBy(window(df.timestamp, \"1 minute\")).agg(\n",
    "    collect_list(struct(\"name\", \"price\")).alias(\"stocks\")\n",
    ")\n",
    "\n",
    "# Define a UDF (User Defined Function) to calculate the total value of each stock\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "@udf(returnType=FloatType())\n",
    "def calculate_max_value(stocks):\n",
    "    return max(stock.price for stock in stocks)\n",
    "\n",
    "# Calculate the total value for each stock within each window\n",
    "windowed_df = windowed_df.withColumn(\"total_value\", calculate_total_value(col(\"stocks\")))\n",
    "\n",
    "# Select the N most valuable stocks within each window\n",
    "n = 5  # Define the value of N\n",
    "most_valuable_stocks = windowed_df.select(\"window\", slice(col(\"stocks\"), 1, n).alias(\"5 most valuable stocks and their prices\"))\n",
    "\n",
    "# Output the results to the console for real-time display\n",
    "query = most_valuable_stocks.writeStream.outputMode(\"complete\").format(\"console\").option(\"truncate\", False).start()\n",
    "# Start the streaming query\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4bef25",
   "metadata": {},
   "source": [
    "## Select the stocks that lost value between two windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, lit\n",
    "# Define a watermark for event-time processing\n",
    "df = df.withWatermark(\"timestamp\", \"10 minutes\")\n",
    "\n",
    "# Create a previous DataFrame to join with the current DataFrame\n",
    "previous_df = df.withColumnRenamed(\"name\", \"prev_name\") \\\n",
    "    .withColumnRenamed(\"timestamp\", \"prev_timestamp\") \\\n",
    "    .withColumnRenamed(\"price\", \"prev_price\")\n",
    "\n",
    "# Join the current and previous DataFrames\n",
    "df_with_price_difference = df.join(\n",
    "    previous_df,\n",
    "    (df[\"name\"] == previous_df[\"prev_name\"]) & (df[\"timestamp\"] > previous_df[\"prev_timestamp\"]),\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Calculate price_difference\n",
    "df_with_price_difference = df_with_price_difference.withColumn(\"price_difference\", col(\"price\") - col(\"prev_price\"))\n",
    "\n",
    "# Filter for rows with a price loss\n",
    "df_with_loss = df_with_price_difference.filter(col(\"price_difference\") <= 0)\n",
    "df_to_output = df_with_loss.select(\"name\", \"price\", \"timestamp\", \"prev_price\", \"prev_timestamp\", \"price_difference\")\n",
    "\n",
    "# Output the results to the console\n",
    "output_query = df_with_loss.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Start the streaming query\n",
    "output_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5deceb8",
   "metadata": {},
   "source": [
    "## Select the stock that gained the most (between windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756e416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lag, window, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Function to handle each batch of data\n",
    "def process_batch(df, epoch_id):\n",
    "    window_spec = Window.partitionBy(\"name\").orderBy(\"window\")\n",
    "\n",
    "    # Create a column \"previous_price\" by using the lag function\n",
    "    df = df.withColumn(\"PreviousAverage\", lag(\"avg(price)\").over(window_spec))\n",
    "    df = df.withColumn(\"previous_window\", lag(\"window\").over(window_spec))\n",
    "    # Calculate the difference and add a new column\n",
    "   \n",
    "\n",
    "    # Filter where there is a previous price and the previous price is greater than the current avg price\n",
    "    df = df.filter(col(\"PreviousAverage\").isNotNull() & (col(\"PreviousAverage\") < col(\"avg(price)\")))\n",
    "    df = df.withColumn(\"HighestGain\", col(\"avg(price)\") - col(\"PreviousAverage\"))\n",
    "    df = df.orderBy(col(\"HighestGain\").desc())\n",
    "\n",
    "    # Take only the stock with the biggest increase\n",
    "    df = df.limit(1)\n",
    "    # Proceed with data processing here, for example: print to the screen\n",
    "    df.select(\"name\", \"window\", \"avg(price)\", \"PreviousAverage\", \"previous_window\", \"HighestGain\").show(truncate=False)\n",
    "\n",
    "windowedDF_2 = df \\\n",
    "    .withWatermark(\"timestamp\", \"3 seconds\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\"), \"name\") \\\n",
    "    .agg({\"price\": \"avg\"})\n",
    "\n",
    "# No need to sort here as we're not writing out the sorted results, sorting will happen in process_batch if needed\n",
    "# lost_value_stocks = windowedDF_2.orderBy(\"avg(price)\", ascending=False)\n",
    "\n",
    "query_2 = (windowedDF_2.writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"TheStocksThatLostValue1\")\n",
    "    .foreachBatch(process_batch)\n",
    "    .start())\n",
    "\n",
    "query_2.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded963a",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27098204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col, first, last\n",
    "\n",
    "windowed_data = df.withWatermark(\"timestamp\", \"1 hour\").groupBy(\n",
    "    window(\"timestamp\", \"1 hour\"),\n",
    "    col(\"name\")\n",
    ").agg(\n",
    "    (((first(df.price) - last(df.price)) / first(df.price)) * 100).alias(\"percentage_change\")\n",
    ")\n",
    "\n",
    "# Define the threshold for acceptable percentage change\n",
    "threshold = 5  # For example, a 5% change threshold\n",
    "\n",
    "# Filter the data for stocks that did not lose too much value\n",
    "control_pass = windowed_data.filter(col(\"percentage_change\") >= -threshold)\n",
    "\n",
    "# Start the streaming query\n",
    "query = (control_pass\n",
    "    .writeStream\n",
    "    .outputMode(\"update\")\n",
    "    .format(\"console\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13183c06",
   "metadata": {},
   "source": [
    "## Compute your assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4573fb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/06 20:18:18 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-abe18fab-9eb1-47fe-8c2e-7e78bb6839a3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------+\n",
      "|correlation|\n",
      "+-----------+\n",
      "|       null|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------+\n",
      "|correlation|\n",
      "+-----------+\n",
      "|       null|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------+\n",
      "|correlation|\n",
      "+-----------+\n",
      "|       null|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/06 20:23:29 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from dc4eaebee9b9:46857 in 10000 milliseconds\n",
      "23/11/06 20:23:29 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0-1, groupId=spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0] Error connecting to node kafka1:9092 (id: 2147483646 rack: null)\n",
      "java.net.UnknownHostException: kafka1: Temporary failure in name resolution\n",
      "\tat java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929)\n",
      "\tat java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1330)\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1283)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1199)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.tryConnect(ConsumerNetworkClient.java:572)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:757)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:737)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:599)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:409)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1210)\n",
      "23/11/06 20:23:29 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0-1, groupId=spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0] Error connecting to node kafka1:9092 (id: 2147483646 rack: null)\n",
      "java.net.UnknownHostException: kafka1\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1287)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1199)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1210)\n",
      "23/11/06 20:23:29 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0-1, groupId=spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0] Error connecting to node kafka1:9092 (id: 2147483646 rack: null)\n",
      "java.net.UnknownHostException: kafka1\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1287)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1199)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.tryConnect(ConsumerNetworkClient.java:572)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:757)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:737)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:599)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:409)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1210)\n",
      "23/11/06 20:23:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0-1, groupId=spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0] Error connecting to node kafka1:9092 (id: 2147483646 rack: null)\n",
      "java.net.UnknownHostException: kafka1\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1287)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1199)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.tryConnect(ConsumerNetworkClient.java:572)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:757)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:737)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:599)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:409)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1210)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------+\n",
      "|correlation|\n",
      "+-----------+\n",
      "|       null|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/06 20:23:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0-1, groupId=spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0] Error connecting to node kafka1:9092 (id: 2147483646 rack: null)\n",
      "java.net.UnknownHostException: kafka1\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1287)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1199)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.tryConnect(ConsumerNetworkClient.java:572)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:757)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:737)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:599)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:409)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:212)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:230)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchLatestOffsets(KafkaOffsetReader.scala:316)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:380)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:371)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:128)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:368)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:208)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\n",
      "23/11/06 20:23:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0-1, groupId=spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0] Error connecting to node kafka1:9092 (id: 2147483646 rack: null)\n",
      "java.net.UnknownHostException: kafka1\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1287)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1199)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.tryConnect(ConsumerNetworkClient.java:572)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:757)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:737)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:599)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:409)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:212)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:230)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchLatestOffsets(KafkaOffsetReader.scala:316)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:380)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:371)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:128)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:368)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:208)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\n",
      "23/11/06 20:23:33 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0-1, groupId=spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0] Error connecting to node kafka1:9092 (id: 2147483646 rack: null)\n",
      "java.net.UnknownHostException: kafka1\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1287)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1199)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.tryConnect(ConsumerNetworkClient.java:572)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:757)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:737)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:599)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:409)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:212)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:230)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchLatestOffsets(KafkaOffsetReader.scala:316)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:380)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:371)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:128)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:368)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:208)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\n",
      "23/11/06 20:23:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0-1, groupId=spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0] Error connecting to node kafka1:9092 (id: 2147483646 rack: null)\n",
      "java.net.UnknownHostException: kafka1\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1287)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1199)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.tryConnect(ConsumerNetworkClient.java:572)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:757)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:737)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:599)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:409)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:212)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:230)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchLatestOffsets(KafkaOffsetReader.scala:316)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:380)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:371)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:128)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:368)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:208)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\n",
      "23/11/06 20:23:35 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0-1, groupId=spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0] Error connecting to node kafka1:9092 (id: 2147483646 rack: null)\n",
      "java.net.UnknownHostException: kafka1\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1287)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1199)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.tryConnect(ConsumerNetworkClient.java:572)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:757)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:737)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:599)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:409)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:212)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:230)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchLatestOffsets(KafkaOffsetReader.scala:316)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:380)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:371)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:128)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:368)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:208)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\n",
      "23/11/06 20:23:36 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0-1, groupId=spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0] 1 partitions have leader brokers without a matching listener, including [stock-0]\n",
      "23/11/06 20:23:36 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0-1, groupId=spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0] Error connecting to node kafka1:9092 (id: 2147483646 rack: null)\n",
      "java.net.UnknownHostException: kafka1\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1287)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1199)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.tryConnect(ConsumerNetworkClient.java:572)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:757)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:737)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:599)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:409)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:212)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:230)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchLatestOffsets(KafkaOffsetReader.scala:316)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:380)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:371)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:128)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:368)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:208)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\n",
      "23/11/06 20:23:37 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0-1, groupId=spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0] Error connecting to node kafka1:9092 (id: 2147483646 rack: null)\n",
      "java.net.UnknownHostException: kafka1\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1287)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1199)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.tryConnect(ConsumerNetworkClient.java:572)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:757)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:737)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:599)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:409)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:212)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:230)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchLatestOffsets(KafkaOffsetReader.scala:316)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:380)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:371)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:128)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:368)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:208)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\n",
      "23/11/06 20:23:38 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0-1, groupId=spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0] Error connecting to node kafka1:9092 (id: 2147483646 rack: null)\n",
      "java.net.UnknownHostException: kafka1\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1287)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1199)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.tryConnect(ConsumerNetworkClient.java:572)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:757)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:737)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:599)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:409)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:212)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:230)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchLatestOffsets(KafkaOffsetReader.scala:316)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:380)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:371)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:128)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:368)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:208)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----------+\n",
      "|correlation|\n",
      "+-----------+\n",
      "|       null|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/06 20:27:06 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0-1, groupId=spark-kafka-source-f720b012-de44-4ee3-afa0-14f452e0f986--108891843-driver-0] Error connecting to node kafka1:9092 (id: 2147483646 rack: null)\n",
      "java.net.UnknownHostException: kafka1: Temporary failure in name resolution\n",
      "\tat java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929)\n",
      "\tat java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1330)\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1283)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1199)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.tryConnect(ConsumerNetworkClient.java:572)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:757)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:737)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:599)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:409)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1210)\n",
      "23/11/06 20:27:43 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 154007 ms exceeds timeout 120000 ms\n",
      "23/11/06 20:27:43 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----------+\n",
      "|correlation|\n",
      "+-----------+\n",
      "|       null|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, corr\n",
    "# Define the schema for your stocks ownership data\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True)  # Represents the total value of the stocks owned by Aida\n",
    "])\n",
    "\n",
    "# Sample stocks ownership data with the total value of the stocks owned by Aida\n",
    "data = [(\"Aida\", 50.0), \n",
    "        (\"Aida\", 75.0), \n",
    "        (\"Aida\", 36.0)]  \n",
    "\n",
    "# Create a DataFrame with the ownership data\n",
    "stocks_ownership_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Select the relevant columns from the parsed Kafka data and alias them\n",
    "df = lines.select(\n",
    "    col(\"parsed_value.name\").alias(\"stock_name\"),\n",
    "    col(\"parsed_value.price\").alias(\"stock_price\")\n",
    ")\n",
    "\n",
    "# Join your ownership data with the streaming data using an inner join\n",
    "merged_df = df.join(stocks_ownership_df, df.stock_name == stocks_ownership_df.name, \"inner\")\n",
    "\n",
    "# Calculate the correlation between your assets and each stock's price\n",
    "correlation_result = merged_df.select(corr(\"price\", \"stock_price\").alias(\"correlation\"))\n",
    "\n",
    "# Start the streaming query to continuously monitor changes in asset value and the correlation\n",
    "query = (correlation_result\n",
    "    .writeStream\n",
    "    .outputMode(\"update\")\n",
    "    .format(\"console\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8233f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
