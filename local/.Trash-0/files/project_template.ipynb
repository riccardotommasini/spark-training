{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0590c248",
   "metadata": {},
   "source": [
    "## Project Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5eccce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-01be8b2c-2386-42dc-9ff3-05d93bcce7fc;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 172ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-01be8b2c-2386-42dc-9ff3-05d93bcce7fc\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/3ms)\n",
      "23/12/18 08:42:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61a67f5",
   "metadata": {},
   "source": [
    "Be sure to start the stream on Kafka!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "314359f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType, DateType\n",
    "\n",
    "schema = StructType(\n",
    "      [\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"price\", DoubleType(), False),\n",
    "        StructField(\"timestamp\", TimestampType(), False),\n",
    "      ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4866c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_server = \"kafka1:9092\"   \n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "lines = (spark.readStream                        # Get the DataStreamReader\n",
    "  .format(\"kafka\")                                 # Specify the source format as \"kafka\"\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_server) # Configure the Kafka server name and port\n",
    "  .option(\"subscribe\", \"stock\")                       # Subscribe to the \"en\" Kafka topic \n",
    "  .option(\"startingOffsets\", \"earliest\")           # The start point when a query is started\n",
    "  .option(\"maxOffsetsPerTrigger\", 100)             # Rate limit on max offsets per trigger interval\n",
    "  .load()\n",
    "  .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\"))\n",
    "# Load the DataFrame\n",
    ")\n",
    "df = lines.select(\"parsed_value.*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e529b1",
   "metadata": {},
   "source": [
    "## The assignment starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91096613",
   "metadata": {},
   "source": [
    "## Select the N most valuable stocks in a window (TASK 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0112c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group the data by a 20-minute window and stock name, and calculate the average price\n",
    "windowedDF = df.groupBy(\n",
    "    window(col(\"timestamp\"), \"20 minutes\"),\n",
    "    \"name\"\n",
    ").avg(\"price\").withColumnRenamed(\"avg(price)\", \"avg_price\")\n",
    "\n",
    "# Now, since you want the 5 most valuable stocks, we will have to do a ranking\n",
    "# For streaming DataFrame, we would need to write the results in update mode to a query sink that supports updates (like a memory sink or a Delta table)\n",
    "\n",
    "# We need to define an aggregation query\n",
    "topStocksQuery = windowedDF \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .foreachBatch(lambda df, epoch_id: df.orderBy(F.desc(\"avg_price\")).limit(5).show()) \\\n",
    "    .start()\n",
    "\n",
    "topStocksQuery.awaitTermination(timeout=5)\n",
    "topStocksQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff9bee3",
   "metadata": {},
   "source": [
    "## Select the stocks that lost value between two windows (TASK 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68199f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col, last\n",
    "from pyspark.sql.types import StringType, DoubleType, StructType, StructField, TimestampType\n",
    "\n",
    "\n",
    "# Use a dictionary to hold the state of each stock\n",
    "stock_states = {}\n",
    "# Set to hold names of stocks that lost value\n",
    "stocks_that_lost_value = set()\n",
    "\n",
    "# Define a class to hold the state of each stock\n",
    "class StockState:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.last_price = None\n",
    "        self.last_window_end = None\n",
    "        self.lost_value = False  # Add flag to track if the stock lost value\n",
    "\n",
    "    def update(self, price, window_end):\n",
    "        if self.last_window_end is None or window_end > self.last_window_end:\n",
    "            if self.last_price is not None and price < self.last_price:\n",
    "                self.lost_value = True # Stock lost value\n",
    "                stocks_that_lost_value.add(self.name)\n",
    "            else:\n",
    "                self.lost_value = False  # Stock did not lose value\n",
    "            self.last_price = price\n",
    "            self.last_window_end = window_end\n",
    "        # Return the state including whether the stock lost value\n",
    "        return (self.name, self.last_price, self.last_window_end, self.lost_value)\n",
    "\n",
    "\n",
    "# Define the watermark and window operation on the streaming DataFrame\n",
    "windowedDF = df \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        col(\"name\"),\n",
    "        window(col(\"timestamp\"), \"20 minutes\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        last(\"price\").alias(\"last_price\")\n",
    "    )\n",
    "\n",
    "def process_batch(df, epoch_id):\n",
    "    # Process the DataFrame row by row\n",
    "    for row in df.collect():\n",
    "        stock_name = row['name']\n",
    "        price = row['last_price']\n",
    "        window_end = row['window'].end\n",
    "\n",
    "        # Get the stock state or create it if it doesn't exist\n",
    "        stock_state = stock_states.get(stock_name, StockState(stock_name))\n",
    "        \n",
    "        # Update the state and check if the stock lost value\n",
    "        result = stock_state.update(price, window_end)\n",
    "        stock_states[stock_name] = stock_state  # Update the state in the dictionary\n",
    "\n",
    "    print(stocks_that_lost_value)\n",
    "\n",
    "# Define the streaming query using the process_batch function\n",
    "query = windowedDF.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()  # Start the streaming query\n",
    "\n",
    "# Await termination of the streaming query (this is running continuously)\n",
    "query.awaitTermination(timeout=60)\n",
    "query.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286a03c",
   "metadata": {},
   "source": [
    "## Select the stock that gained the most (between windows) (TASK 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c49cbd",
   "metadata": {},
   "source": [
    "## TASK 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6136aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 08:42:56 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-815fcdaa-3740-4d2a-a0de-517f28e6ef56. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LYB', 19.099999999999994), ('A', 16.82), ('FISV', 13.925000000000004), ('ETFC', 11.35), ('ADM', 8.509999999999998), ('AFL', 8.350000000000001), ('MAR', 8.079999999999998), ('BBT', 6.669999999999998), ('CHK', 5.800000000000001), ('APH', 5.515000000000001)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('REGN', 104.67599999999999), ('ALXN', 72.17999999999999), ('BLK', 49.95999999999998), ('HII', 49.519999999999996), ('ORLY', 36.235), ('EOG', 32.010000000000005), ('MLM', 31.36), ('PKG', 30.0), ('CMI', 23.519899999999993), ('GWW', 23.430000000000007)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AZO', 133.99), ('REGN', 104.67599999999999), ('ALXN', 72.17999999999999), ('BLK', 49.95999999999998), ('HII', 49.519999999999996), ('AAP', 45.59), ('NOC', 42.43000000000001), ('WDC', 40.5), ('ORLY', 36.235), ('FB', 32.56)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AZO', 133.99), ('REGN', 104.67599999999999), ('ALXN', 72.17999999999999), ('BLK', 49.95999999999998), ('HII', 49.519999999999996), ('AAP', 45.59), ('NOC', 42.43000000000001), ('WDC', 40.5), ('LLL', 36.34), ('ORLY', 36.235)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 80\u001b[0m\n\u001b[1;32m     74\u001b[0m query \u001b[38;5;241m=\u001b[39m windowedDF\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(process_batch) \\\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()  \u001b[38;5;66;03m# Start the streaming query\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Await termination of the streaming query (this is running continuously)\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#query.stop()\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/streaming.py:101\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(timeout, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout must be a positive integer or float. Got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m timeout)\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1303\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1296\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1033\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1200\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1200\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1201\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[0;32m/usr/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=========================>                             (91 + 6) / 200]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col, last\n",
    "\n",
    "# Define a class to hold the state of each stock\n",
    "class StockState:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.last_price = None\n",
    "        self.last_window_end = None\n",
    "\n",
    "    def update(self, new_price, window_end):\n",
    "        if self.last_window_end is None or window_end > self.last_window_end:\n",
    "            # Calculate the gain only if it's a new window and we have a previous price\n",
    "            if self.last_price is not None:\n",
    "                gain = new_price - self.last_price\n",
    "                self.last_price = new_price\n",
    "                self.last_window_end = window_end\n",
    "                return gain  # Return the gain to be collected\n",
    "            else:\n",
    "                # If there's no previous price, just update the state\n",
    "                self.last_price = new_price\n",
    "                self.last_window_end = window_end\n",
    "                return 0  # Return zero gain\n",
    "        else:\n",
    "            # If it's not a new window, do nothing\n",
    "            return 0  # Return zero gain\n",
    "\n",
    "\n",
    "\n",
    "# Assuming df is a DataFrame with the schema: [timestamp, name, price]\n",
    "# Define the watermark and window operation on the streaming DataFrame\n",
    "windowedDF = df \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        col(\"name\"),\n",
    "        window(col(\"timestamp\"), \"20 minutes\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        last(\"price\").alias(\"last_price\")\n",
    "    )\n",
    "\n",
    "# Use a dictionary to hold the state of each stock\n",
    "stock_states = {}\n",
    "# Set to hold tuples of (stock_name, gain)\n",
    "gains_set = set()\n",
    "\n",
    "def process_batch(df, epoch_id):\n",
    "    # Temporary list to hold gains for sorting\n",
    "    gains_list = []\n",
    "\n",
    "    # Process the DataFrame row by row\n",
    "    for row in df.collect():\n",
    "        stock_name = row['name']\n",
    "        price = row['last_price']\n",
    "        window_end = row['window'].end\n",
    "\n",
    "        # Get the stock state or create it if it doesn't exist\n",
    "        stock_state = stock_states.get(stock_name, StockState(stock_name))\n",
    "\n",
    "        # Update the state and collect the gain\n",
    "        gain = stock_state.update(price, window_end)\n",
    "        if gain > 0:\n",
    "            gains_list.append((stock_name, gain))\n",
    "        stock_states[stock_name] = stock_state  # Update the state in the dictionary\n",
    "\n",
    "    # Update the set with new gains and sort it\n",
    "    gains_set.update(gains_list)\n",
    "    # Sort the set by gain in descending order and take the top 10\n",
    "    top_gainers = sorted(gains_set, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    print(top_gainers)\n",
    "\n",
    "# Define the streaming query using the process_batch function\n",
    "query = windowedDF.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()  # Start the streaming query\n",
    "\n",
    "# Await termination of the streaming query (this is running continuously)\n",
    "query.awaitTermination(timeout=60)\n",
    "#query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bee1a8",
   "metadata": {},
   "source": [
    "## TASK 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad65596",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca98689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col, first, last\n",
    "\n",
    "# Defining the windowed data with appropriate watermark\n",
    "windowed_data = df.withWatermark(\"timestamp\", \"1 hour\") \\\n",
    "                  .groupBy(window(\"timestamp\", \"1 hour\"), col(\"name\")) \\\n",
    "                  .agg((((first(df.price) - last(df.price)) / first(df.price)) * 100).alias(\"value_change\"))\n",
    "\n",
    "# Defining the threshold for acceptable percentage change\n",
    "threshold = 5  # A 5% change threshold is used in this example\n",
    "\n",
    "# Filtering the data for stocks that did not lose too much value\n",
    "control_pass = windowed_data.filter(col(\"value_change\") >= -threshold)\n",
    "\n",
    "# Starting the streaming query\n",
    "query = control_pass.writeStream \\\n",
    "                .outputMode(\"update\") \\\n",
    "                .format(\"console\") \\\n",
    "                .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e7fa7",
   "metadata": {},
   "source": [
    "## Bonus(Influx db & Gafana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab4ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def generate_window_id(window_start):\n",
    "    # This function assumes window_start is a string like '2023-10-29 14:40:00'\n",
    "    # Replace non-numeric characters with nothing to create a numeric string\n",
    "    numeric_string = window_start.replace('-', '').replace(' ', '').replace(':', '')\n",
    "    \n",
    "    # Alternatively, use a hash function to generate a shorter, unique identifier\n",
    "    # window_id = hashlib.md5(window_start.encode()).hexdigest()[:8]  # Use the first 8 characters\n",
    "\n",
    "    return numeric_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d61daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import influxdb_client\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "from pyspark.sql import Row\n",
    "import time\n",
    "\n",
    "class InfluxDBForeachWriter:\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.url = \"http://influxdb:8086\"\n",
    "        self.token = \"uX9_2aP6otzrpPG6XRwZ8LPShIEtFbtiwU2Yya0HfC8fCWyMbHZ7Xm-ivXo7on2MYaPqEEwJL2TAtRU-O_n56A==\"\n",
    "        self.org = \"streaming_practice\"\n",
    "        self.bucket = \"stocks\"\n",
    "        self.client = influxdb_client.InfluxDBClient(\n",
    "            url=self.url,\n",
    "            token=self.token,\n",
    "            org=self.org\n",
    "        )\n",
    "        self.write_api = self.client.write_api(write_options=SYNCHRONOUS)\n",
    "        return True\n",
    "\n",
    "    def process(self, row: Row,window_id:int):\n",
    "        try:\n",
    "            # Convert timestamp to the correct format for InfluxDB (RFC3339)\n",
    "            # Assuming timestamp is a string in the format 'YYYY-MM-DD HH:MM:SS'\n",
    "\n",
    "            window_start_str = row.window.start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            # Prepare data for InfluxDB\n",
    "            data_point = influxdb_client.Point(\"stock_testing\") \\\n",
    "                .tag(\"name\", str(row.name)) \\\n",
    "                .tag(\"window_id\",window_id) \\\n",
    "                .field(\"avg_price\", float(row.avg_price)) \\\n",
    "                .field(\"window_start\", window_start_str) \\\n",
    "                .time(int(time.time_ns()))  # Use the current time in nanoseconds\n",
    "            print(f\"Attempting to write data point: {data_point.to_line_protocol()}\")\n",
    "\n",
    "            # Write data to InfluxDB\n",
    "            self.write_api.write(bucket=self.bucket, record=data_point)\n",
    "            print(\"Write successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    def close(self, error):\n",
    "        # Close the InfluxDB client\n",
    "        self.write_api.close()\n",
    "        self.client.close()\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c20d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.streaming import DataStreamWriter\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the window_id globally\n",
    "window_id = 0\n",
    "\n",
    "def write_to_influxdb(df, epoch_id):\n",
    "    # Use the global keyword to indicate that you are using the global window_id variable\n",
    "    global window_id\n",
    "\n",
    "    # Instantiate the InfluxDBForeachWriter outside the loop\n",
    "    writer = InfluxDBForeachWriter()\n",
    "\n",
    "    if writer.open(epoch_id, None):  # Open the connection to InfluxDB using the epoch_id\n",
    "        print(window_id)\n",
    "        # Sort by average price and take the top 5\n",
    "        top5_df = df.orderBy(F.desc(\"avg_price\")).limit(5).collect()\n",
    "\n",
    "        for row in top5_df:\n",
    "            try:\n",
    "                # Pass window_id along with the row data\n",
    "                writer.process(row, window_id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing to InfluxDB: {e}\")\n",
    "                break  # or continue, depending on your requirement\n",
    "\n",
    "        # Increment window_id for the next batch of data\n",
    "        window_id += 1\n",
    "        writer.close(epoch_id)  # Close the connection after all rows are written\n",
    "    else:\n",
    "        print(\"Failed to open InfluxDB connection\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "writer = InfluxDBForeachWriter()\n",
    "\n",
    "windowedDF = df.groupBy(\n",
    "    window(col(\"timestamp\"), \"20 minutes\"),\n",
    "    \"name\"\n",
    ").avg(\"price\").withColumnRenamed(\"avg(price)\", \"avg_price\")\n",
    "\n",
    "# Define the streaming query using foreachBatch\n",
    "topStocksQuery = windowedDF \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .foreachBatch(write_to_influxdb) \\\n",
    "    .start()\n",
    "\n",
    "topStocksQuery.awaitTermination(timeout=50)\n",
    "topStocksQuery.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
