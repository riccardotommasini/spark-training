{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6565e3f8",
   "metadata": {
    "id": "6565e3f8"
   },
   "source": [
    "## Project Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "jdmQFdy9a6Js",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60214,
     "status": "ok",
     "timestamp": 1699099819393,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "jdmQFdy9a6Js",
    "outputId": "66f43ac7-b8e2-46f4-c220-0664a3d0880d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=058b58a0fc73761760cb44d6eaf391b9fa4ef5cd45f853f4a5e7f9c4064f54ab\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55974225",
   "metadata": {
    "executionInfo": {
     "elapsed": 24983,
     "status": "ok",
     "timestamp": 1699099851763,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "55974225"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-cf458a8f-0f9e-4b22-80a0-49504de7bd31;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 171ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-cf458a8f-0f9e-4b22-80a0-49504de7bd31\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/4ms)\n",
      "23/12/18 10:02:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35005b8",
   "metadata": {
    "id": "a35005b8"
   },
   "source": [
    "Be sure to start the stream on Kafka!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e622d5b-ef4d-4d78-abcb-e9edc2305ac8",
   "metadata": {
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1699100172627,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "4e622d5b-ef4d-4d78-abcb-e9edc2305ac8"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType, DateType\n",
    "\n",
    "schema = StructType(\n",
    "      [\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"price\", DoubleType(), False),\n",
    "        StructField(\"timestamp\", TimestampType(), False),\n",
    "      ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69712d38",
   "metadata": {
    "executionInfo": {
     "elapsed": 3994,
     "status": "ok",
     "timestamp": 1699100179724,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "69712d38"
   },
   "outputs": [],
   "source": [
    "kafka_server = \"kafka1:9092\"\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "lines = (spark.readStream                        # Get the DataStreamReader\n",
    "  .format(\"kafka\")                                 # Specify the source format as \"kafka\"\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_server) # Configure the Kafka server name and port\n",
    "  .option(\"subscribe\", \"stock\")                       # Subscribe to the \"en\" Kafka topic\n",
    "  .option(\"startingOffsets\", \"earliest\")           # The start point when a query is started\n",
    "  .option(\"maxOffsetsPerTrigger\", 100)             # Rate limit on max offsets per trigger interval\n",
    "  .load()\n",
    "  .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\"))\n",
    "# Load the DataFrame\n",
    ")\n",
    "df = lines.select(\"parsed_value.*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a46c68-44ab-4e3a-90fb-d334423e4acc",
   "metadata": {
    "id": "74a46c68-44ab-4e3a-90fb-d334423e4acc"
   },
   "source": [
    "## The assignment starts here\n",
    "\n",
    "You can create a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24187ef-e5b4-4fa7-bab5-60aa94412a05",
   "metadata": {
    "id": "c24187ef-e5b4-4fa7-bab5-60aa94412a05"
   },
   "source": [
    "## Select the N most valuable stocks in a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65b2d7ed",
   "metadata": {
    "executionInfo": {
     "elapsed": 439,
     "status": "ok",
     "timestamp": 1699100182831,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "65b2d7ed",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, sum, rank\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "xiuClqi7r0FQ",
   "metadata": {
    "executionInfo": {
     "elapsed": 663,
     "status": "ok",
     "timestamp": 1699100185679,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "xiuClqi7r0FQ"
   },
   "outputs": [],
   "source": [
    "#Create a SparkSession\n",
    "Spark=SparkSession.builder.appName(\"StockDashboard\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "VAVj2cIbvvwb",
   "metadata": {
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1699100187831,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "VAVj2cIbvvwb"
   },
   "outputs": [],
   "source": [
    "#Define an appropriate window duration and the number of most valuable stocks to select\n",
    "windowDuration=\"5 minutes\"\n",
    "N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "yiVw2m14tRXH",
   "metadata": {
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1699100192137,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "yiVw2m14tRXH"
   },
   "outputs": [],
   "source": [
    "#Grouping the data\n",
    "windowed_data=df.groupBy(window(\"timestamp\", windowDuration), \"name\").agg(sum(\"price\").alias(\"total_value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "jKOku9_9tVTw",
   "metadata": {
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1699100194243,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "jKOku9_9tVTw"
   },
   "outputs": [],
   "source": [
    "#rank the data\n",
    "Window_spec=Window.partitionBy(\"window\").orderBy(col(\"total_value\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3V5amlZVtX8C",
   "metadata": {
    "executionInfo": {
     "elapsed": 1221,
     "status": "ok",
     "timestamp": 1699100197076,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "3V5amlZVtX8C"
   },
   "outputs": [],
   "source": [
    "#Creation of new column named rank\n",
    "Ranked_data=windowed_data.withColumn(\"rank\", rank().over(Window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7afFDM0Ztacd",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1699100198129,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "7afFDM0Ztacd"
   },
   "outputs": [],
   "source": [
    "#Filter the Rank column\n",
    "Most_Valuable_Stocks=Ranked_data.filter(col(\"rank\")<= N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9tsLD0v1tdHI",
   "metadata": {
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1699100202675,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "9tsLD0v1tdHI"
   },
   "outputs": [],
   "source": [
    "#Create an SQL view\n",
    "windowed_data.createOrReplaceTempView(\"windowed_stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "jhrojrPTtfEL",
   "metadata": {
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1699100204648,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "jhrojrPTtfEL"
   },
   "outputs": [],
   "source": [
    "#SQl format\n",
    "sql_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM windowed_stocks\n",
    "    ORDER BY total_value DESC\n",
    "    LIMIT {N}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "FedIW7bOthIx",
   "metadata": {
    "executionInfo": {
     "elapsed": 368,
     "status": "ok",
     "timestamp": 1699100207164,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "FedIW7bOthIx"
   },
   "outputs": [],
   "source": [
    "#executing the SQL query using Spark SQL\n",
    "most_valuable_stocks = spark.sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "VVv5D-OxtjN0",
   "metadata": {
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1699100210106,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "VVv5D-OxtjN0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 09:46:48 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-abeb58b8-07f7-449b-8b3b-6b775dd9f6f8. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "query = most_valuable_stocks.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5EXJcvFtwvl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1699100213360,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "f5EXJcvFtwvl",
    "outputId": "7ca19657-166f-4db1-a1e0-96d858c465f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.awaitTermination(timeout=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8DJyDnUEtyrs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 396,
     "status": "error",
     "timestamp": 1699100264637,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "8DJyDnUEtyrs",
    "outputId": "8dad614b-d439-494a-d3b8-bf984c427509"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: query; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [query]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM query\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/session.py:646\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;129m@ignore_unicode_prefix\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m2.0\u001b[39m)\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    637\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m    :return: :class:`DataFrame`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py:137\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    133\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: query; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [query]\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"SELECT * FROM query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "KmVwd0KQt1W-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 678,
     "status": "error",
     "timestamp": 1699100221818,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "KmVwd0KQt1W-",
    "outputId": "61842779-b200-4400-8e65-9cae46612b48"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746caef-fc7c-4d0e-98df-cdd6046393eb",
   "metadata": {
    "id": "6746caef-fc7c-4d0e-98df-cdd6046393eb"
   },
   "source": [
    "## Select the stocks that lost value between two windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bea404b-fc76-48f9-83d9-5946617863de",
   "metadata": {
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1699100794164,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "6bea404b-fc76-48f9-83d9-5946617863de"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, rank, lag, col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import window, sum, rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "iZPy7srcK9Ij",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1699100795951,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "iZPy7srcK9Ij"
   },
   "outputs": [],
   "source": [
    "#Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"StockMarketAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "QaL47-QuLAxj",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1699100797292,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "QaL47-QuLAxj"
   },
   "outputs": [],
   "source": [
    "# Define the window duration and N value\n",
    "Window_Duration = \"5 minutes\"\n",
    "N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nvKgJ1Uk-pNc",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1699100799164,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "nvKgJ1Uk-pNc"
   },
   "outputs": [],
   "source": [
    "# Define a watermark to handle late data\n",
    "windowed_data = df.withWatermark(\"timestamp\", \"5 minutes\").groupBy(window(\"timestamp\", Window_Duration), \"name\").agg(sum(\"price\").alias(\"total_value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "SOqdIyjs-_8A",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1699100802315,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "SOqdIyjs-_8A"
   },
   "outputs": [],
   "source": [
    "# Define a window specification for ranking stocks by total value\n",
    "windowSpec = Window.partitionBy(\"window\").orderBy(col(\"total_value\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5EyQs9Gu_DmI",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1699100803958,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "5EyQs9Gu_DmI"
   },
   "outputs": [],
   "source": [
    "# Rank the stocks by total value within each time window\n",
    "ranked_data = windowed_data.withColumn(\"rank\", rank().over(windowSpec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aiEOggqL_GcN",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1699100805408,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "aiEOggqL_GcN"
   },
   "outputs": [],
   "source": [
    "# Filter and select the top N most valuable stocks in each window\n",
    "most_valuable_stocks = ranked_data.filter(col(\"rank\") <= N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "DmnGIan6_Jlr",
   "metadata": {
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1699100807465,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "DmnGIan6_Jlr"
   },
   "outputs": [],
   "source": [
    "# Create a temporary view for querying\n",
    "most_valuable_stocks.createOrReplaceTempView(\"most_valuable_stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "RH4KxVwq_ME0",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1699100808046,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "RH4KxVwq_ME0"
   },
   "outputs": [],
   "source": [
    "# Define a SQL query to select stocks that lost value between windows\n",
    "sql_query = \"\"\"\n",
    "    SELECT a.window AS window, a.name AS name, (a.total_value - b.total_value) AS value_change\n",
    "    FROM most_valuable_stocks a\n",
    "    LEFT JOIN most_valuable_stocks b\n",
    "    ON a.window = b.window AND a.name = b.name\n",
    "    WHERE a.rank = 1 AND b.rank = 2\n",
    "    AND (a.total_value - b.total_value) < 0\n",
    "    ORDER BY a.window, a.name\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tlUgCRol_PWM",
   "metadata": {
    "executionInfo": {
     "elapsed": 589,
     "status": "ok",
     "timestamp": 1699100811563,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "tlUgCRol_PWM"
   },
   "outputs": [],
   "source": [
    "# Create a streaming DataFrame from the query\n",
    "losing_stocks_stream = spark.sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05ENNa7V_-Hv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "error",
     "timestamp": 1699100812992,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "05ENNa7V_-Hv",
    "outputId": "bf647bed-a021-4997-c64b-5f914ba3282e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 09:55:05 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a86aa21f-5710-425a-80d8-7cfa63c8f632. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Multiple streaming aggregations are not supported with streaming DataFrames/Datasets;;\nProject [window#52-T300000ms, name#53, value_change#54]\n+- Sort [window#29-T300000ms ASC NULLS FIRST, name#23 ASC NULLS FIRST], true\n   +- Project [window#29-T300000ms AS window#52-T300000ms, name#23 AS name#53, (total_value#34 - total_value#60) AS value_change#54, window#29-T300000ms, name#23]\n      +- Filter (((rank#41 = 1) AND (rank#55 = 2)) AND ((total_value#34 - total_value#60) < cast(0 as double)))\n         +- Join LeftOuter, ((window#29-T300000ms = window#59-T300000ms) AND (name#23 = name#67))\n            :- SubqueryAlias a\n            :  +- SubqueryAlias most_valuable_stocks\n            :     +- Filter (rank#41 <= 5)\n            :        +- Project [window#29-T300000ms, name#23, total_value#34, rank#41]\n            :           +- Project [window#29-T300000ms, name#23, total_value#34, rank#41, rank#41]\n            :              +- Window [rank(total_value#34) windowspecdefinition(window#29-T300000ms, total_value#34 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#41], [window#29-T300000ms], [total_value#34 DESC NULLS LAST]\n            :                 +- Project [window#29-T300000ms, name#23, total_value#34]\n            :                    +- Aggregate [window#35-T300000ms, name#23], [window#35-T300000ms AS window#29-T300000ms, name#23, sum(price#24) AS total_value#34]\n            :                       +- Filter isnotnull(timestamp#25-T300000ms)\n            :                          +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#25-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#25-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#25-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#25-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#25-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#25-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#25-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#25-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0) + 300000000), LongType, TimestampType)) AS window#35-T300000ms, name#23, price#24, timestamp#25-T300000ms]\n            :                             +- EventTimeWatermark timestamp#25: timestamp, 5 minutes\n            :                                +- Project [parsed_value#21.name AS name#23, parsed_value#21.price AS price#24, parsed_value#21.timestamp AS timestamp#25]\n            :                                   +- Project [from_json(StructField(name,StringType,false), StructField(price,DoubleType,false), StructField(timestamp,TimestampType,false), cast(value#8 as string), Some(Etc/UTC)) AS parsed_value#21]\n            :                                      +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@7a265cba, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@9321180, org.apache.spark.sql.util.CaseInsensitiveStringMap@89a1c66b, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@326aa66d,kafka,List(),None,List(),None,Map(maxOffsetsPerTrigger -> 100, startingOffsets -> earliest, subscribe -> stock, kafka.bootstrap.servers -> kafka1:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n            +- SubqueryAlias b\n               +- SubqueryAlias most_valuable_stocks\n                  +- Filter (rank#55 <= 5)\n                     +- Project [window#59-T300000ms, name#67, total_value#60, rank#55]\n                        +- Project [window#59-T300000ms, name#67, total_value#60, rank#55, rank#55]\n                           +- Window [rank(total_value#60) windowspecdefinition(window#59-T300000ms, total_value#60 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#55], [window#59-T300000ms], [total_value#60 DESC NULLS LAST]\n                              +- Project [window#59-T300000ms, name#67, total_value#60]\n                                 +- Aggregate [window#35-T300000ms, name#67], [window#35-T300000ms AS window#59-T300000ms, name#67, sum(price#68) AS total_value#60]\n                                    +- Filter isnotnull(timestamp#69-T300000ms)\n                                       +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#69-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#69-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#69-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#69-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#69-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#69-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#69-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#69-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0) + 300000000), LongType, TimestampType)) AS window#35-T300000ms, name#67, price#68, timestamp#69-T300000ms]\n                                          +- EventTimeWatermark timestamp#69: timestamp, 5 minutes\n                                             +- Project [parsed_value#21.name AS name#67, parsed_value#21.price AS price#68, parsed_value#21.timestamp AS timestamp#69]\n                                                +- Project [from_json(StructField(name,StringType,false), StructField(price,DoubleType,false), StructField(timestamp,TimestampType,false), cast(value#8 as string), Some(Etc/UTC)) AS parsed_value#21]\n                                                   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@7a265cba, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@9321180, org.apache.spark.sql.util.CaseInsensitiveStringMap@89a1c66b, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@326aa66d,kafka,List(),None,List(),None,Map(maxOffsetsPerTrigger -> 100, startingOffsets -> earliest, subscribe -> stock, kafka.bootstrap.servers -> kafka1:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Write the streaming results to a file (you can use any supported output format)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43mlosing_stocks_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlosing_stocks_stream_query\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/streaming.py:1211\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py:137\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    133\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Multiple streaming aggregations are not supported with streaming DataFrames/Datasets;;\nProject [window#52-T300000ms, name#53, value_change#54]\n+- Sort [window#29-T300000ms ASC NULLS FIRST, name#23 ASC NULLS FIRST], true\n   +- Project [window#29-T300000ms AS window#52-T300000ms, name#23 AS name#53, (total_value#34 - total_value#60) AS value_change#54, window#29-T300000ms, name#23]\n      +- Filter (((rank#41 = 1) AND (rank#55 = 2)) AND ((total_value#34 - total_value#60) < cast(0 as double)))\n         +- Join LeftOuter, ((window#29-T300000ms = window#59-T300000ms) AND (name#23 = name#67))\n            :- SubqueryAlias a\n            :  +- SubqueryAlias most_valuable_stocks\n            :     +- Filter (rank#41 <= 5)\n            :        +- Project [window#29-T300000ms, name#23, total_value#34, rank#41]\n            :           +- Project [window#29-T300000ms, name#23, total_value#34, rank#41, rank#41]\n            :              +- Window [rank(total_value#34) windowspecdefinition(window#29-T300000ms, total_value#34 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#41], [window#29-T300000ms], [total_value#34 DESC NULLS LAST]\n            :                 +- Project [window#29-T300000ms, name#23, total_value#34]\n            :                    +- Aggregate [window#35-T300000ms, name#23], [window#35-T300000ms AS window#29-T300000ms, name#23, sum(price#24) AS total_value#34]\n            :                       +- Filter isnotnull(timestamp#25-T300000ms)\n            :                          +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#25-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#25-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#25-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#25-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#25-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#25-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#25-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#25-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0) + 300000000), LongType, TimestampType)) AS window#35-T300000ms, name#23, price#24, timestamp#25-T300000ms]\n            :                             +- EventTimeWatermark timestamp#25: timestamp, 5 minutes\n            :                                +- Project [parsed_value#21.name AS name#23, parsed_value#21.price AS price#24, parsed_value#21.timestamp AS timestamp#25]\n            :                                   +- Project [from_json(StructField(name,StringType,false), StructField(price,DoubleType,false), StructField(timestamp,TimestampType,false), cast(value#8 as string), Some(Etc/UTC)) AS parsed_value#21]\n            :                                      +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@7a265cba, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@9321180, org.apache.spark.sql.util.CaseInsensitiveStringMap@89a1c66b, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@326aa66d,kafka,List(),None,List(),None,Map(maxOffsetsPerTrigger -> 100, startingOffsets -> earliest, subscribe -> stock, kafka.bootstrap.servers -> kafka1:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n            +- SubqueryAlias b\n               +- SubqueryAlias most_valuable_stocks\n                  +- Filter (rank#55 <= 5)\n                     +- Project [window#59-T300000ms, name#67, total_value#60, rank#55]\n                        +- Project [window#59-T300000ms, name#67, total_value#60, rank#55, rank#55]\n                           +- Window [rank(total_value#60) windowspecdefinition(window#59-T300000ms, total_value#60 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#55], [window#59-T300000ms], [total_value#60 DESC NULLS LAST]\n                              +- Project [window#59-T300000ms, name#67, total_value#60]\n                                 +- Aggregate [window#35-T300000ms, name#67], [window#35-T300000ms AS window#59-T300000ms, name#67, sum(price#68) AS total_value#60]\n                                    +- Filter isnotnull(timestamp#69-T300000ms)\n                                       +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#69-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#69-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#69-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#69-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#69-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#69-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#69-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#69-T300000ms, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0) + 300000000), LongType, TimestampType)) AS window#35-T300000ms, name#67, price#68, timestamp#69-T300000ms]\n                                          +- EventTimeWatermark timestamp#69: timestamp, 5 minutes\n                                             +- Project [parsed_value#21.name AS name#67, parsed_value#21.price AS price#68, parsed_value#21.timestamp AS timestamp#69]\n                                                +- Project [from_json(StructField(name,StringType,false), StructField(price,DoubleType,false), StructField(timestamp,TimestampType,false), cast(value#8 as string), Some(Etc/UTC)) AS parsed_value#21]\n                                                   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@7a265cba, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@9321180, org.apache.spark.sql.util.CaseInsensitiveStringMap@89a1c66b, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@326aa66d,kafka,List(),None,List(),None,Map(maxOffsetsPerTrigger -> 100, startingOffsets -> earliest, subscribe -> stock, kafka.bootstrap.servers -> kafka1:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n"
     ]
    }
   ],
   "source": [
    "# Write the streaming results to a file (you can use any supported output format)\n",
    "query = losing_stocks_stream.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"losing_stocks_stream_query\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268c285-d55c-4be3-8e5a-e7ddebb14153",
   "metadata": {
    "id": "8268c285-d55c-4be3-8e5a-e7ddebb14153"
   },
   "source": [
    "## Select the stock that gained the most (between windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7ad21-59c2-4d4c-befe-9d1ceedbb74d",
   "metadata": {
    "id": "b7e7ad21-59c2-4d4c-befe-9d1ceedbb74d"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, sum, rank, col\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B6ZGUPsNBIGJ",
   "metadata": {
    "id": "B6ZGUPsNBIGJ"
   },
   "outputs": [],
   "source": [
    "#Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"StockAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V6GrmgMtBPS_",
   "metadata": {
    "id": "V6GrmgMtBPS_"
   },
   "outputs": [],
   "source": [
    "# Define the window duration and the number of stocks to select\n",
    "Window_Duration = \"5 minutes\"\n",
    "N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eXVBj0WdBTI4",
   "metadata": {
    "id": "eXVBj0WdBTI4"
   },
   "outputs": [],
   "source": [
    "# Calculate the total value of stocks in each window\n",
    "windowed_data = df.groupBy(window(\"timestamp\", Window_Duration), \"name\").agg(sum(\"price\").alias(\"total_value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O_4l6ywlBo0g",
   "metadata": {
    "id": "O_4l6ywlBo0g"
   },
   "outputs": [],
   "source": [
    "# Define a window specification for ranking stocks by total value\n",
    "Window_spec = Window.partitionBy(\"window\").orderBy(col(\"total_value\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AYgBFVNqBrUb",
   "metadata": {
    "id": "AYgBFVNqBrUb"
   },
   "outputs": [],
   "source": [
    "#Rank the stocks based on total value within each window\n",
    "Ranked_data = windowed_data.withColumn(\"rank\", rank().over(Window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5csIImZMBueD",
   "metadata": {
    "id": "5csIImZMBueD"
   },
   "outputs": [],
   "source": [
    "# Select the most valuable stocks\n",
    "most_valuable_stocks = Ranked_data.filter(col(\"rank\") <= N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QX4LGfR_BxC9",
   "metadata": {
    "id": "QX4LGfR_BxC9"
   },
   "outputs": [],
   "source": [
    "# Create a temporary view for querying\n",
    "windowed_data.createOrReplaceTempView(\"windowed_stocks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "tB4DUO_HBzyl",
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1699100906051,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "tB4DUO_HBzyl"
   },
   "outputs": [],
   "source": [
    "# Define a SQL query to find the stocks that gained the most value between windows\n",
    "sql_query = f\"\"\"\n",
    "    SELECT a.window AS window, a.name AS name, (a.total_value - b.total_value) AS value_change\n",
    "    FROM most_valuable_stocks a\n",
    "    LEFT JOIN most_valuable_stocks b\n",
    "    ON a.window = b.window AND a.name = b.name\n",
    "    WHERE a.rank = 1 AND b.rank = 2\n",
    "    AND (a.total_value - b.total_value) > 0\n",
    "    ORDER BY a.window, a.name\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drd7Z70vB2Hj",
   "metadata": {
    "id": "drd7Z70vB2Hj"
   },
   "outputs": [],
   "source": [
    "# Create a streaming DataFrame from the query\n",
    "gaining_stocks_stream = spark.sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pIaHPIPOB4-3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "executionInfo": {
     "elapsed": 402,
     "status": "error",
     "timestamp": 1699023453555,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "pIaHPIPOB4-3",
    "outputId": "2a33d969-0de2-486e-cf35-1df3de553a27"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-863e50bfe9f6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gaining_stocks_stream_query\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/readwriter.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [NON_TIME_WINDOW_NOT_SUPPORTED_IN_STREAMING] Window function is not supported in RANK(TOTAL_VALUE#88) (as column `rank`) on streaming DataFrames/Datasets. Structured Streaming only supports time-window aggregation using the WINDOW function. (window specification: (PARTITION BY WINDOW ORDER BY TOTAL_VALUE DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW))"
     ]
    }
   ],
   "source": [
    "# Write the streaming results to a file (you can use any supported output format)\n",
    "query = gaining_stocks_stream.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"gaining_stocks_stream_query\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ARXuP2kAYyeJ",
   "metadata": {
    "id": "ARXuP2kAYyeJ"
   },
   "source": [
    "Implement a control that checks if a stock does not lose too much value in a period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ZTpqgLceYt3Q",
   "metadata": {
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1699102293208,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "ZTpqgLceYt3Q"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, sum, col\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"StockValueLossMonitor\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "TVI3mP0_Y4Lq",
   "metadata": {
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1699102313264,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "TVI3mP0_Y4Lq"
   },
   "outputs": [],
   "source": [
    "# Define window duration and threshold for value loss\n",
    "window_duration = \"30 minutes\"\n",
    "threshold_value_loss = 0.05  # 5% loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8q6BLgcvY8_j",
   "metadata": {
    "executionInfo": {
     "elapsed": 440,
     "status": "ok",
     "timestamp": 1699102343159,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "8q6BLgcvY8_j"
   },
   "outputs": [],
   "source": [
    "# Window data and calculate total value\n",
    "windowed_data = df.groupBy(window(\"timestamp\", window_duration), \"name\").agg(sum(\"price\").alias(\"total_value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ohK_mC16ZEY_",
   "metadata": {
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1699102409947,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "ohK_mC16ZEY_"
   },
   "outputs": [],
   "source": [
    "# Define the window specification for ranking stocks by total value\n",
    "windowSpec = Window.partitionBy(\"window\").orderBy(col(\"total_value\").desc())\n",
    "\n",
    "# Calculate value changes within each window\n",
    "value_changes = windowed_data.withColumn(\"lag_total_value\", lag(\"total_value\").over(windowSpec))\n",
    "value_changes = value_changes.withColumn(\"value_change\", (col(\"total_value\") - col(\"lag_total_value\")) / col(\"total_value\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "vHJv7uPlZUyd",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1699102428328,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "vHJv7uPlZUyd"
   },
   "outputs": [],
   "source": [
    "# Filter stocks with significant value loss\n",
    "significant_value_loss = value_changes.filter(col(\"value_change\") < -threshold_value_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "Vtw5bUNvZosx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "executionInfo": {
     "elapsed": 391,
     "status": "error",
     "timestamp": 1699102531782,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "Vtw5bUNvZosx",
    "outputId": "237f8a93-6dbd-4708-f9cc-c95db3d0904b"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-df2ca271f7d9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define a watermark to handle late data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwindowed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindowed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithWatermark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"5 minutes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Filter significant value losses (for example, losses greater than -0.05 or 5%)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msignificant_value_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_changes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value_change\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithWatermark\u001b[0;34m(self, eventTime, delayThreshold)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 },\n\u001b[1;32m   1142\u001b[0m             )\n\u001b[0;32m-> 1143\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithWatermark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meventTime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelayThreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `timestamp` cannot be resolved. Did you mean one of the following? [`name`, `window`, `total_value`].;\n'EventTimeWatermark 'timestamp, 5 minutes\n+- Aggregate [window#207, name#23], [window#207 AS window#201, name#23, sum(price#24) AS total_value#206]\n   +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#25, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#25, TimestampType, LongType) - 0) % 1800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#25, TimestampType, LongType) - 0) % 1800000000) + 1800000000) ELSE ((precisetimestampconversion(timestamp#25, TimestampType, LongType) - 0) % 1800000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#25, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#25, TimestampType, LongType) - 0) % 1800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#25, TimestampType, LongType) - 0) % 1800000000) + 1800000000) ELSE ((precisetimestampconversion(timestamp#25, TimestampType, LongType) - 0) % 1800000000) END) - 0) + 1800000000), LongType, TimestampType))) AS window#207, name#23, price#24, timestamp#25]\n      +- Filter isnotnull(timestamp#25)\n         +- Project [parsed_value#21.name AS name#23, parsed_value#21.price AS price#24, parsed_value#21.timestamp AS timestamp#25]\n            +- Project [from_json(StructField(name,StringType,false), StructField(price,DoubleType,false), StructField(timestamp,TimestampType,false), cast(value#8 as string), Some(Etc/UTC)) AS parsed_value#21]\n               +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@527a263, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@235aaede, [startingOffsets=earliest, kafka.bootstrap.servers=kafka1:9092, subscribe=stock, maxOffsetsPerTrigger=100], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@5f0aade1,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9092, subscribe -> stock, startingOffsets -> earliest, maxOffsetsPerTrigger -> 100),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n"
     ]
    }
   ],
   "source": [
    "# Define a watermark to handle late data\n",
    "windowed_data = windowed_data.withWatermark(\"timestamp\", \"5 minutes\")\n",
    "\n",
    "# Filter significant value losses (for example, losses greater than -0.05 or 5%)\n",
    "significant_value_loss = value_changes.filter(col(\"value_change\") < -0.05)\n",
    "\n",
    "# Start the streaming query\n",
    "query = significant_value_loss.writeStream.outputMode(\"append\").format(\"memory\").queryName(\"significant_value_loss_query\").start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b-gDrBVb-2a",
   "metadata": {
    "id": "0b-gDrBVb-2a"
   },
   "source": [
    "Compute your assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fJO13TkncCuQ",
   "metadata": {
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1699103127759,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "fJO13TkncCuQ"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session in Google Colab\n",
    "spark = SparkSession.builder.appName(\"AssetFluctuationAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e-kz7IsCcD6Y",
   "metadata": {
    "executionInfo": {
     "elapsed": 1568,
     "status": "ok",
     "timestamp": 1699103168597,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "e-kz7IsCcD6Y"
   },
   "outputs": [],
   "source": [
    "# Assume you have a DataFrame 'portfolio' with schema <name, amount of stocks owned>\n",
    "# Replace this with your actual portfolio data\n",
    "portfolio = spark.createDataFrame([(\"AAPL\", 10), (\"GOOGL\", 5), (\"AMZN\", 7)], [\"name\", \"amount\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exrumlMYcN0u",
   "metadata": {
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1699103181777,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "exrumlMYcN0u"
   },
   "outputs": [],
   "source": [
    "# Deserialize data from Kafka (assuming it's in JSON format)\n",
    "\n",
    "# Join portfolio data with market data by stock name\n",
    "joinedData = df.join(portfolio, \"name\", \"left\")\n",
    "\n",
    "# Calculate asset value based on the current stock price and amount owned\n",
    "assetValue = joinedData.withColumn(\"asset_value\", col(\"price\") * col(\"amount\"))\n",
    "\n",
    "# Group data by timestamp and calculate the total asset value\n",
    "assetFluctuation = assetValue.groupBy(\"timestamp\").agg({\"asset_value\": \"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "VBWBlpELcRIy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "executionInfo": {
     "elapsed": 465,
     "status": "error",
     "timestamp": 1699103201020,
     "user": {
      "displayName": "Μαλβίνα Μιχαλουδη",
      "userId": "07238130301644761614"
     },
     "user_tz": -120
    },
    "id": "VBWBlpELcRIy",
    "outputId": "77115697-41e6-4077-b780-91129379be85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 10:03:06 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ab2d3122-1906-40a7-b0b3-57c61f6ae010. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------+----------------+\n",
      "|          timestamp|sum(asset_value)|\n",
      "+-------------------+----------------+\n",
      "|2023-12-15 15:30:50|            null|\n",
      "|2023-12-15 15:30:44|            null|\n",
      "|2023-12-15 15:30:28|            null|\n",
      "|2023-12-15 15:30:20|            null|\n",
      "|2023-12-15 15:30:30|            null|\n",
      "|2023-12-15 15:30:06|            null|\n",
      "|2023-12-15 15:30:18|            null|\n",
      "|2023-12-15 15:30:16|            null|\n",
      "|2023-12-15 15:30:37|            null|\n",
      "|2023-12-15 15:30:51|            null|\n",
      "|2023-12-15 15:30:21|            null|\n",
      "|2023-12-15 15:30:46|            null|\n",
      "|2023-12-15 15:30:45|            null|\n",
      "|2023-12-15 15:30:17|            null|\n",
      "|2023-12-15 15:30:10|            null|\n",
      "|2023-12-15 15:30:52|            null|\n",
      "|2023-12-15 15:30:11|            null|\n",
      "|2023-12-15 15:30:26|            null|\n",
      "|2023-12-15 15:30:39|            null|\n",
      "|2023-12-15 15:30:09|            null|\n",
      "+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------------------+----------------+\n",
      "|          timestamp|sum(asset_value)|\n",
      "+-------------------+----------------+\n",
      "|2023-12-15 15:30:50|            null|\n",
      "|2023-12-15 15:30:44|            null|\n",
      "|2023-12-15 15:30:28|            null|\n",
      "|2023-12-15 15:30:20|            null|\n",
      "|2023-12-15 15:30:30|            null|\n",
      "|2023-12-15 15:30:06|            null|\n",
      "|2023-12-15 15:30:16|            null|\n",
      "|2023-12-15 15:30:18|            null|\n",
      "|2023-12-15 15:30:37|            null|\n",
      "|2023-12-15 15:31:15|            null|\n",
      "|2023-12-15 15:31:34|            null|\n",
      "|2023-12-15 15:30:51|            null|\n",
      "|2023-12-15 15:31:11|            null|\n",
      "|2023-12-15 15:30:21|            null|\n",
      "|2023-12-15 15:30:46|            null|\n",
      "|2023-12-15 15:30:45|            null|\n",
      "|2023-12-15 15:30:17|            null|\n",
      "|2023-12-15 15:31:29|            null|\n",
      "|2023-12-15 15:31:23|            null|\n",
      "|2023-12-15 15:31:19|            null|\n",
      "+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------------------+----------------+\n",
      "|          timestamp|sum(asset_value)|\n",
      "+-------------------+----------------+\n",
      "|2023-12-15 15:32:30|            null|\n",
      "|2023-12-15 15:30:50|            null|\n",
      "|2023-12-15 15:30:44|            null|\n",
      "|2023-12-15 15:30:28|            null|\n",
      "|2023-12-15 15:30:20|            null|\n",
      "|2023-12-15 15:30:30|            null|\n",
      "|2023-12-15 15:30:06|            null|\n",
      "|2023-12-15 15:30:16|            null|\n",
      "|2023-12-15 15:30:18|            null|\n",
      "|2023-12-15 15:32:20|            null|\n",
      "|2023-12-15 15:32:32|            null|\n",
      "|2023-12-15 15:32:22|            null|\n",
      "|2023-12-15 15:30:37|            null|\n",
      "|2023-12-15 15:31:15|            null|\n",
      "|2023-12-15 15:31:34|            null|\n",
      "|2023-12-15 15:30:51|            null|\n",
      "|2023-12-15 15:31:11|            null|\n",
      "|2023-12-15 15:30:21|            null|\n",
      "|2023-12-15 15:30:46|            null|\n",
      "|2023-12-15 15:31:50|            null|\n",
      "+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------------------+----------------+\n",
      "|          timestamp|sum(asset_value)|\n",
      "+-------------------+----------------+\n",
      "|2023-12-15 15:32:30|            null|\n",
      "|2023-12-15 15:30:50|            null|\n",
      "|2023-12-15 15:33:06|            null|\n",
      "|2023-12-15 15:30:44|            null|\n",
      "|2023-12-15 15:30:28|            null|\n",
      "|2023-12-15 15:30:20|            null|\n",
      "|2023-12-15 15:32:58|            null|\n",
      "|2023-12-15 15:30:30|            null|\n",
      "|2023-12-15 15:30:06|            null|\n",
      "|2023-12-15 15:33:19|            null|\n",
      "|2023-12-15 15:30:18|            null|\n",
      "|2023-12-15 15:30:16|            null|\n",
      "|2023-12-15 15:32:20|            null|\n",
      "|2023-12-15 15:32:32|            null|\n",
      "|2023-12-15 15:32:22|            null|\n",
      "|2023-12-15 15:30:37|            null|\n",
      "|2023-12-15 15:31:15|            null|\n",
      "|2023-12-15 15:31:34|            null|\n",
      "|2023-12-15 15:30:51|            null|\n",
      "|2023-12-15 15:31:11|            null|\n",
      "+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------------------+----------------+\n",
      "|          timestamp|sum(asset_value)|\n",
      "+-------------------+----------------+\n",
      "|2023-12-15 15:32:30|            null|\n",
      "|2023-12-15 15:30:50|            null|\n",
      "|2023-12-15 15:33:06|            null|\n",
      "|2023-12-15 15:30:44|            null|\n",
      "|2023-12-15 15:30:28|            null|\n",
      "|2023-12-15 15:33:28|            null|\n",
      "|2023-12-15 15:30:20|            null|\n",
      "|2023-12-15 15:32:58|            null|\n",
      "|2023-12-15 15:30:30|            null|\n",
      "|2023-12-15 15:30:06|            null|\n",
      "|2023-12-15 15:33:19|            null|\n",
      "|2023-12-15 15:30:18|            null|\n",
      "|2023-12-15 15:30:16|            null|\n",
      "|2023-12-15 15:32:20|            null|\n",
      "|2023-12-15 15:33:45|            null|\n",
      "|2023-12-15 15:32:32|            null|\n",
      "|2023-12-15 15:33:35|            null|\n",
      "|2023-12-15 15:33:51|            null|\n",
      "|2023-12-15 15:32:22|            null|\n",
      "|2023-12-15 15:30:37|            null|\n",
      "+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-------------------+----------------+\n",
      "|          timestamp|sum(asset_value)|\n",
      "+-------------------+----------------+\n",
      "|2023-12-15 15:32:30|            null|\n",
      "|2023-12-15 15:30:50|            null|\n",
      "|2023-12-15 15:33:06|            null|\n",
      "|2023-12-15 15:30:44|            null|\n",
      "|2023-12-15 15:30:28|            null|\n",
      "|2023-12-15 15:34:21|            null|\n",
      "|2023-12-15 15:33:28|            null|\n",
      "|2023-12-15 15:30:20|            null|\n",
      "|2023-12-15 15:32:58|            null|\n",
      "|2023-12-15 15:34:40|            null|\n",
      "|2023-12-15 15:30:30|            null|\n",
      "|2023-12-15 15:30:06|            null|\n",
      "|2023-12-15 15:33:19|            null|\n",
      "|2023-12-15 15:30:18|            null|\n",
      "|2023-12-15 15:30:16|            null|\n",
      "|2023-12-15 15:32:20|            null|\n",
      "|2023-12-15 15:33:45|            null|\n",
      "|2023-12-15 15:32:32|            null|\n",
      "|2023-12-15 15:33:35|            null|\n",
      "|2023-12-15 15:33:51|            null|\n",
      "+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-------------------+----------------+\n",
      "|          timestamp|sum(asset_value)|\n",
      "+-------------------+----------------+\n",
      "|2023-12-15 15:33:06|            null|\n",
      "|2023-12-15 15:35:55|            null|\n",
      "|2023-12-15 15:32:30|            null|\n",
      "|2023-12-15 15:30:50|            null|\n",
      "|2023-12-15 15:30:44|            null|\n",
      "|2023-12-15 15:30:28|            null|\n",
      "|2023-12-15 15:34:21|            null|\n",
      "|2023-12-15 15:33:28|            null|\n",
      "|2023-12-15 15:30:20|            null|\n",
      "|2023-12-15 15:32:58|            null|\n",
      "|2023-12-15 15:34:40|            null|\n",
      "|2023-12-15 15:30:30|            null|\n",
      "|2023-12-15 15:30:06|            null|\n",
      "|2023-12-15 15:33:19|            null|\n",
      "|2023-12-15 15:30:18|            null|\n",
      "|2023-12-15 15:30:16|            null|\n",
      "|2023-12-15 15:32:20|            null|\n",
      "|2023-12-15 15:33:45|            null|\n",
      "|2023-12-15 15:32:32|            null|\n",
      "|2023-12-15 15:35:09|            null|\n",
      "+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-------------------+----------------+\n",
      "|          timestamp|sum(asset_value)|\n",
      "+-------------------+----------------+\n",
      "|2023-12-15 15:33:06|            null|\n",
      "|2023-12-15 15:35:55|            null|\n",
      "|2023-12-15 15:32:30|            null|\n",
      "|2023-12-15 15:30:50|            null|\n",
      "|2023-12-15 15:36:24|            null|\n",
      "|2023-12-15 15:30:44|            null|\n",
      "|2023-12-15 15:30:28|            null|\n",
      "|2023-12-15 15:34:21|            null|\n",
      "|2023-12-15 15:33:28|            null|\n",
      "|2023-12-15 15:30:20|            null|\n",
      "|2023-12-15 15:36:07|            null|\n",
      "|2023-12-15 15:32:58|            null|\n",
      "|2023-12-15 15:34:40|            null|\n",
      "|2023-12-15 15:30:30|            null|\n",
      "|2023-12-15 15:30:06|            null|\n",
      "|2023-12-15 15:36:29|            null|\n",
      "|2023-12-15 15:36:06|            null|\n",
      "|2023-12-15 15:33:19|            null|\n",
      "|2023-12-15 15:30:18|            null|\n",
      "|2023-12-15 15:30:16|            null|\n",
      "+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-------------------+----------------+\n",
      "|          timestamp|sum(asset_value)|\n",
      "+-------------------+----------------+\n",
      "|2023-12-15 15:33:06|            null|\n",
      "|2023-12-15 15:35:55|            null|\n",
      "|2023-12-15 15:32:30|            null|\n",
      "|2023-12-15 15:30:50|            null|\n",
      "|2023-12-15 15:36:24|            null|\n",
      "|2023-12-15 15:30:44|            null|\n",
      "|2023-12-15 15:30:28|            null|\n",
      "|2023-12-15 15:34:21|            null|\n",
      "|2023-12-15 15:33:28|            null|\n",
      "|2023-12-15 15:30:20|            null|\n",
      "|2023-12-15 15:36:07|            null|\n",
      "|2023-12-15 15:32:58|            null|\n",
      "|2023-12-15 15:34:40|            null|\n",
      "|2023-12-15 15:30:30|            null|\n",
      "|2023-12-15 15:30:06|            null|\n",
      "|2023-12-15 15:36:29|            null|\n",
      "|2023-12-15 15:36:06|            null|\n",
      "|2023-12-15 15:33:19|            null|\n",
      "|2023-12-15 15:30:18|            null|\n",
      "|2023-12-15 15:30:16|            null|\n",
      "+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+-------------------+----------------+\n",
      "|          timestamp|sum(asset_value)|\n",
      "+-------------------+----------------+\n",
      "|2023-12-15 15:33:06|            null|\n",
      "|2023-12-15 15:35:55|            null|\n",
      "|2023-12-15 15:32:30|            null|\n",
      "|2023-12-15 15:30:50|            null|\n",
      "|2023-12-15 15:36:24|            null|\n",
      "|2023-12-15 15:30:44|            null|\n",
      "|2023-12-15 15:30:28|            null|\n",
      "|2023-12-15 15:34:21|            null|\n",
      "|2023-12-15 15:33:28|            null|\n",
      "|2023-12-15 15:30:20|            null|\n",
      "|2023-12-15 15:36:07|            null|\n",
      "|2023-12-15 15:32:58|            null|\n",
      "|2023-12-15 15:38:24|            null|\n",
      "|2023-12-15 15:34:40|            null|\n",
      "|2023-12-15 15:30:30|            null|\n",
      "|2023-12-15 15:30:06|            null|\n",
      "|2023-12-15 15:36:29|            null|\n",
      "|2023-12-15 15:38:20|            null|\n",
      "|2023-12-15 15:36:06|            null|\n",
      "|2023-12-15 15:33:19|            null|\n",
      "+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+-------------------+----------------+\n",
      "|          timestamp|sum(asset_value)|\n",
      "+-------------------+----------------+\n",
      "|2023-12-15 15:33:06|            null|\n",
      "|2023-12-15 15:35:55|            null|\n",
      "|2023-12-15 15:32:30|            null|\n",
      "|2023-12-15 15:30:50|            null|\n",
      "|2023-12-15 15:39:07|            null|\n",
      "|2023-12-15 15:36:24|            null|\n",
      "|2023-12-15 15:30:44|            null|\n",
      "|2023-12-15 15:30:28|            null|\n",
      "|2023-12-15 15:34:21|            null|\n",
      "|2023-12-15 15:33:28|            null|\n",
      "|2023-12-15 15:30:20|            null|\n",
      "|2023-12-15 15:36:07|            null|\n",
      "|2023-12-15 15:32:58|            null|\n",
      "|2023-12-15 15:38:24|            null|\n",
      "|2023-12-15 15:34:40|            null|\n",
      "|2023-12-15 15:30:30|            null|\n",
      "|2023-12-15 15:30:06|            null|\n",
      "|2023-12-15 15:38:20|            null|\n",
      "|2023-12-15 15:38:48|            null|\n",
      "|2023-12-15 15:36:29|            null|\n",
      "+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+-------------------+----------------+\n",
      "|          timestamp|sum(asset_value)|\n",
      "+-------------------+----------------+\n",
      "|2023-12-15 15:33:06|            null|\n",
      "|2023-12-15 15:35:55|            null|\n",
      "|2023-12-15 15:32:30|            null|\n",
      "|2023-12-15 15:30:50|            null|\n",
      "|2023-12-15 15:39:07|            null|\n",
      "|2023-12-15 15:36:24|            null|\n",
      "|2023-12-15 15:30:44|            null|\n",
      "|2023-12-15 15:30:28|            null|\n",
      "|2023-12-15 15:34:21|            null|\n",
      "|2023-12-15 15:33:28|            null|\n",
      "|2023-12-15 15:30:20|            null|\n",
      "|2023-12-15 15:36:07|            null|\n",
      "|2023-12-15 15:32:58|            null|\n",
      "|2023-12-15 15:38:24|            null|\n",
      "|2023-12-15 15:34:40|            null|\n",
      "|2023-12-15 15:30:30|            null|\n",
      "|2023-12-15 15:30:06|            null|\n",
      "|2023-12-15 15:38:20|            null|\n",
      "|2023-12-15 15:38:48|            null|\n",
      "|2023-12-15 15:36:29|            null|\n",
      "+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:==========================================>           (158 + 6) / 200]\r"
     ]
    }
   ],
   "source": [
    "# Define an output sink (e.g., console, memory, or a file)\n",
    "query = assetFluctuation.writeStream.outputMode(\"complete\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd180f7-25e1-4ee1-99fc-7591a6a47e07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1gw_7-mHNhoNS1I6pjR0fSkvyPfxDrBTN",
     "timestamp": 1699293853567
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
