{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ccec28",
   "metadata": {},
   "source": [
    "## Project Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ef3a4",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "To run the code using the same environment as provided for the TP in class, follow these steps:\n",
    "\n",
    "1. Copy the `Project_template.ipynb` and `Kafka-Producer-for-project.ipynb` files, as well as the `stocks.csv`, to the `/local` directory.\n",
    "\n",
    "2. Run the following command:\n",
    "docker-compose -f docker-compose.kafka.yml up\n",
    "\n",
    "This will start the necessary Kafka services.\n",
    "\n",
    "3. Make sure to run the Kafka producer before executing this code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd3bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59c4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bebe24c",
   "metadata": {},
   "source": [
    "Be sure to start the stream on Kafka!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0059163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType, DateType\n",
    "\n",
    "schema = StructType(\n",
    "      [\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"price\", DoubleType(), False),\n",
    "        StructField(\"timestamp\", TimestampType(), False),\n",
    "      ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_server = \"kafka1:9092\"   \n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "lines = (spark.readStream                        # Get the DataStreamReader\n",
    "  .format(\"kafka\")                                 # Specify the source format as \"kafka\"\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_server) # Configure the Kafka server name and port\n",
    "  .option(\"subscribe\", \"stock\")                       # Subscribe to the \"en\" Kafka topic \n",
    "  .option(\"startingOffsets\", \"earliest\")           # The start point when a query is started\n",
    "  .option(\"maxOffsetsPerTrigger\", 100)             # Rate limit on max offsets per trigger interval\n",
    "  .load()\n",
    "  .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\"))\n",
    "# Load the DataFrame\n",
    ")\n",
    "df = lines.select(\"parsed_value.*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8877eb",
   "metadata": {},
   "source": [
    "## Select the N most valuable stocks in a window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7713ed76",
   "metadata": {},
   "source": [
    "## Summary of the query code:\n",
    "- **Time Window Duration:** The `window_duration` variable is set to \"5 minutes,\" specifying the duration of each time window for data analysis.\n",
    "\n",
    "- **Calculate Total Value:** A new column, \"total_value,\" is created in the DataFrame `df` to represent the total value of each stock at any given time.\n",
    "\n",
    "- **Group and Aggregate:** The data is grouped based on two criteria: the time window (defined by the timestamp) and the stock's name. The code calculates the sum of the \"total_value\" within each group, aggregating the total value for each stock within each time window.\n",
    "\n",
    "- **Sort Results:** The `windowed_df` DataFrame is sorted in descending order based on the \"total_value\" column, ensuring that the most valuable stocks within each time window appear at the top of the results.\n",
    "\n",
    "- **Display Results:** To view the analysis results, the code utilizes Spark's Structured Streaming to write the sorted DataFrame to the console. The chosen output mode is \"complete,\" meaning that the complete results for each time window are displayed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f426f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, window, sum\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define the time window duration \n",
    "window_duration = \"5 minutes\"\n",
    "\n",
    "# Calculate the total value for each stock within the time window\n",
    "df_with_total_value = df.withColumn(\"total_value\", col(\"price\"))\n",
    "\n",
    "# Group the data by time windows and aggregate within each window\n",
    "windowed_df = df_with_total_value.groupBy(\n",
    "    window(col(\"timestamp\"), window_duration),\n",
    "    col(\"name\")\n",
    ").agg(\n",
    "    sum(col(\"total_value\")).alias(\"total_value\")\n",
    ")\n",
    "\n",
    "# Sort the aggregated DataFrame by total value in descending order\n",
    "sorted_df = windowed_df.orderBy(col(\"total_value\").desc())\n",
    "\n",
    "# Display the results to the console for debugging\n",
    "query = sorted_df.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca91b66",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcdcaad",
   "metadata": {},
   "source": [
    "## Select the stocks that lost value between two windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b2c8fc",
   "metadata": {},
   "source": [
    "## Summary of query 2:\n",
    "- **Group and Aggregate by Time Windows:** The code groups the data by time windows specified by the \"window_duration\" variable and stock name. Within each window, it calculates the price difference, which is the maximum price minus the minimum price for each stock in the given time window.\n",
    "\n",
    "- **Define Stateful Function:** A custom stateful function, \"display_price_difference,\" is defined. This function takes two parameters: \"batch_df,\" representing the DataFrame for a specific batch, and \"batch_id,\" which is the identifier for the batch. The function's purpose is to filter the DataFrame to find stocks that have lost value (price difference < 0) within a given time window.\n",
    "\n",
    "- **Apply Stateful Function with foreachBatch:** The stateful function is applied using the \"foreachBatch\" operation in Structured Streaming. This operation processes each batch of data. When new batches arrive, it filters the DataFrame to identify stocks with a negative price difference (i.e., stocks that lost value) within the defined time windows. The results are displayed using the \"show\" function.\n",
    "\n",
    "- **Start the Streaming Query:** The streaming query is initiated by the \"start\" function, and the code waits for the query to continue processing with \"awaitTermination.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cd66b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, window, max, min\n",
    "window_duration = \"1 minutes\"\n",
    "\n",
    "\n",
    "# Group the data by time windows and stock name and aggregate within each window\n",
    "windowed_df = df.groupBy(\n",
    "    window(\"timestamp\", window_duration),\n",
    "    col(\"name\")\n",
    ").agg(\n",
    "    (max(col(\"price\")) - min(col(\"price\"))).alias(\"price_difference\")\n",
    ")\n",
    "\n",
    "# Define the stateful function to display the price difference\n",
    "def display_price_difference(batch_df, batch_id):\n",
    "    # Filter the DataFrame to find stocks that lost value\n",
    "    loss_df = batch_df.filter(batch_df.price_difference < 0)\n",
    "    loss_df.show()\n",
    "\n",
    "# Apply the stateful function using foreachBatch\n",
    "query = windowed_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .foreachBatch(display_price_difference) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e8023d",
   "metadata": {},
   "source": [
    "# Find the stocks that gained the most between windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f450c",
   "metadata": {},
   "source": [
    "## Summary of query 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67276725",
   "metadata": {},
   "source": [
    "1. Define the time window duration (e.g., 5 minutes).\n",
    "2. Create a windowed DataFrame with the total value for each stock.\n",
    "3. Group the data by time windows and stock name, calculating the average price within each window.\n",
    "4. Define a stateful function to display the average price for each window.\n",
    "5. Apply the stateful function using foreachBatch to display the average price for each window.\n",
    "6. Start the streaming query, and it waits for new data.\n",
    "7. The query runs in complete output mode, showing complete results for each batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edafab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, window, avg\n",
    "\n",
    "# Define the time window duration (e.g., 5 minutes)\n",
    "window_duration = \"1 minutes\"\n",
    "\n",
    "# Create a windowed DataFrame with total value\n",
    "df_with_total_value = df.withColumn(\"total_value\", col(\"price\"))\n",
    "# Group the data by time windows and stock name and aggregate within each window\n",
    "windowed_df = df_with_total_value.groupBy(\n",
    "   window(\"timestamp\", window_duration),\n",
    "   col(\"name\")\n",
    ").agg(\n",
    "   avg(col(\"price\")).alias(\"average_price\")\n",
    ")\n",
    "# Define the stateful function to display the average price\n",
    "def display_average_price(batch_df, batch_id):\n",
    "   batch_df.show()\n",
    "# Apply the stateful function using foreachBatch\n",
    "query = windowed_df.writeStream \\\n",
    "   .outputMode(\"complete\") \\\n",
    "   .foreachBatch(display_average_price) \\\n",
    "   .start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6e90d",
   "metadata": {},
   "source": [
    "# Implement a control that checks if a stock does not lose too much value in a period of time (feel free to choose the value you prefer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77259bd6",
   "metadata": {},
   "source": [
    "## Summary of query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e4104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, window, max, min\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Define the time window duration (e.g., 5 minutes)\n",
    "window_duration = \"5 minutes\"\n",
    "\n",
    "# Define the maximum allowed loss\n",
    "max_loss = -10.0  \n",
    "\n",
    "# Group the data by time windows and stock name and aggregate within each window\n",
    "windowed_df = df.groupBy(\n",
    "    window(\"timestamp\", window_duration),\n",
    "    col(\"name\")\n",
    ").agg(\n",
    "    (max(col(\"price\")) - min(col(\"price\"))).alias(\"price_difference\")\n",
    ")\n",
    "\n",
    "# Define the stateful function to display the stocks that lost too much value\n",
    "def display_large_losses(batch_df, batch_id):\n",
    "    # Filter the DataFrame to find stocks that lost too much value\n",
    "    large_loss_df = batch_df.filter(batch_df.price_difference < max_loss)\n",
    "    large_loss_df.show()\n",
    "\n",
    "# Apply the stateful function using foreachBatch\n",
    "query = windowed_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .foreachBatch(display_large_losses) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe2579",
   "metadata": {},
   "source": [
    "# Compute how your asset changes with the fluctuation of the market"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17bd6f8",
   "metadata": {},
   "source": [
    "## Summary of query 5\n",
    "\n",
    "1. Define a 10-minute time window for data aggregation.\n",
    "2. Group data by stock name and time window, collecting prices into a list.\n",
    "3. Pivot data to create columns for each stock's prices within each time window.\n",
    "4. Fill missing values with 0.\n",
    "5. Use a vector assembler to combine columns into a single vector.\n",
    "6. Calculate correlations between stocks.\n",
    "7. Extract the correlation matrix from the result.\n",
    "8. Choose to save or display the matrix.\n",
    "9. Start a streaming query in complete output mode.\n",
    "10. Continuously display the updated correlation matrix in the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37704811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query 5\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col, window, collect_list, first\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StockMarketAnalysis\").getOrCreate()\n",
    "\n",
    "\n",
    "window_duration = \"10 minutes\"\n",
    "windowed_df = df.groupBy(col(\"name\"), window(col(\"timestamp\"), window_duration)).agg(collect_list(\"price\").alias(\"prices\"))\n",
    "\n",
    "# Pivot the data to have columns for each stock and the price in each window\n",
    "pivoted_df = windowed_df.groupBy(\"window\").pivot(\"name\").agg(first(\"prices\"))\n",
    "pivoted_df = pivoted_df.na.fill(0)  # Fill null values with 0\n",
    "\n",
    "# Create a vector assembler to assemble the columns into a single vector\n",
    "assembler = VectorAssembler(inputCols=pivoted_df.columns[1:], outputCol=\"features\")\n",
    "vectorized_df = assembler.transform(pivoted_df)\n",
    "\n",
    "# Calculate the correlation between stocks\n",
    "correlation_df = vectorized_df.select(\"window\", \"name\", \"features\")\n",
    "correlation_matrix = Correlation.corr(correlation_df, \"features\").head()[0]\n",
    "\n",
    "# Save the correlation matrix to a file or display it\n",
    "correlation_query = correlation_matrix.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Start the streaming query\n",
    "correlation_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ef0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
